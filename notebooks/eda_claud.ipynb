{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e84a19",
   "metadata": {},
   "source": [
    "# Enhanced Geospatial EDA for NYC Taxi Data\n",
    "## Using Dask, GeoPandas, H3, and Advanced Visualization\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on NYC taxi data with focus on geospatial patterns, temporal trends, and advanced visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2670768c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core data processing\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geospatial libraries\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import wkt\n",
    "import contextily as ctx\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster, FastMarkerCluster\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Utilities\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81067821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "class Config:\n",
    "    # Data paths\n",
    "    RAW_TAXI_DATA_PATTERN = \"../data/yellow_tripdata_*.csv\"\n",
    "    TAXI_ZONES_SHAPEFILE = \"../data/taxi_zones/taxi_zones.shp\"\n",
    "    NYC_BOROUGHS_SHAPEFILE = \"../data/boroughs/boroughs.shp\"\n",
    "    \n",
    "    # Column names\n",
    "    PICKUP_DATETIME_COL = 'tpep_pickup_datetime'\n",
    "    DROPOFF_DATETIME_COL = 'tpep_dropoff_datetime'\n",
    "    PICKUP_LAT_COL = 'pickup_latitude'\n",
    "    PICKUP_LON_COL = 'pickup_longitude'\n",
    "    DROPOFF_LAT_COL = 'dropoff_latitude'\n",
    "    DROPOFF_LON_COL = 'dropoff_longitude'\n",
    "    \n",
    "    # H3 configuration\n",
    "    H3_RESOLUTIONS = [7, 8, 9]  # Multiple resolutions for different analyses\n",
    "    H3_MAIN_RESOLUTION = 8\n",
    "    \n",
    "    # NYC boundaries (approximate)\n",
    "    NYC_BOUNDS = {\n",
    "        'lat_min': 40.4774, 'lat_max': 40.9176,\n",
    "        'lon_min': -74.2591, 'lon_max': -73.7004\n",
    "    }\n",
    "    \n",
    "    # Dask configuration\n",
    "    DASK_WORKERS = 4\n",
    "    DASK_THREADS_PER_WORKER = 2\n",
    "    DASK_MEMORY_LIMIT = '4GB'\n",
    "    \n",
    "    # Sampling rates for different analyses\n",
    "    SAMPLE_RATES = {\n",
    "        'visualization': 0.01,\n",
    "        'clustering': 0.005,\n",
    "        'heatmap': 0.001\n",
    "    }\n",
    "\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b26154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 15:39:41,256 - INFO - üöÄ Dask Client initialized: http://127.0.0.1:62124/status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Dashboard: http://127.0.0.1:62124/status\n"
     ]
    }
   ],
   "source": [
    "# Setup Dask Client with optimized configuration\n",
    "def setup_dask_client():\n",
    "    \"\"\"Initialize Dask client with proper cleanup.\"\"\"\n",
    "    try:\n",
    "        # Clean up existing clients\n",
    "        if 'client' in globals() and client:\n",
    "            client.close()\n",
    "        if 'cluster' in globals() and cluster:\n",
    "            cluster.close()\n",
    "    except (NameError, Exception) as e:\n",
    "        logger.debug(f\"No existing client to close: {e}\")\n",
    "    \n",
    "    try:\n",
    "        cluster = LocalCluster(\n",
    "            n_workers=config.DASK_WORKERS,\n",
    "            threads_per_worker=config.DASK_THREADS_PER_WORKER,\n",
    "            memory_limit=config.DASK_MEMORY_LIMIT,\n",
    "            dashboard_address=':8787'\n",
    "        )\n",
    "        client = Client(cluster)\n",
    "        logger.info(f\"üöÄ Dask Client initialized: {client.dashboard_link}\")\n",
    "        return client, cluster\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to initialize Dask client: {e}\")\n",
    "        return None, None\n",
    "\n",
    "client, cluster = setup_dask_client()\n",
    "print(f\"Dask Dashboard: {client.dashboard_link if client else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2ee9ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data processing utilities loaded\n"
     ]
    }
   ],
   "source": [
    "# Utility functions for data processing\n",
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def get_taxi_dtypes():\n",
    "        \"\"\"Define data types for taxi data to ensure proper loading.\"\"\"\n",
    "        return {\n",
    "            'VendorID': 'float64',\n",
    "            'passenger_count': 'float64',\n",
    "            'trip_distance': 'float64',\n",
    "            'RatecodeID': 'float64',\n",
    "            'store_and_fwd_flag': 'object',\n",
    "            config.PICKUP_LON_COL: 'float64',\n",
    "            config.PICKUP_LAT_COL: 'float64',\n",
    "            config.DROPOFF_LON_COL: 'float64',\n",
    "            config.DROPOFF_LAT_COL: 'float64',\n",
    "            'payment_type': 'float64',\n",
    "            'fare_amount': 'float64',\n",
    "            'extra': 'float64',\n",
    "            'mta_tax': 'float64',\n",
    "            'tip_amount': 'float64',\n",
    "            'tolls_amount': 'float64',\n",
    "            'improvement_surcharge': 'float64',\n",
    "            'total_amount': 'float64',\n",
    "            'congestion_surcharge': 'float64',\n",
    "            'airport_fee': 'float64'\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_nyc_bounds(df, lat_col, lon_col):\n",
    "        \"\"\"Filter data to NYC boundaries.\"\"\"\n",
    "        return df[\n",
    "            (df[lat_col].between(config.NYC_BOUNDS['lat_min'], config.NYC_BOUNDS['lat_max'])) &\n",
    "            (df[lon_col].between(config.NYC_BOUNDS['lon_min'], config.NYC_BOUNDS['lon_max']))\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_h3_convert(lat, lon, resolution):\n",
    "        \"\"\"Safely convert lat/lon to H3 hex.\"\"\"\n",
    "        try:\n",
    "            if pd.isna(lat) or pd.isna(lon):\n",
    "                return None\n",
    "            return h3.geo_to_h3(float(lat), float(lon), int(resolution))\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_h3_to_partition(df_partition, lat_col, lon_col, resolution, h3_col_name):\n",
    "        \"\"\"Apply H3 conversion to a Dask partition.\"\"\"\n",
    "        if lat_col not in df_partition.columns or lon_col not in df_partition.columns:\n",
    "            df_partition[h3_col_name] = None\n",
    "            return df_partition\n",
    "        \n",
    "        df_partition[h3_col_name] = df_partition.apply(\n",
    "            lambda row: DataProcessor.safe_h3_convert(row[lat_col], row[lon_col], resolution),\n",
    "            axis=1\n",
    "        )\n",
    "        return df_partition\n",
    "\n",
    "processor = DataProcessor()\n",
    "print(\"‚úÖ Data processing utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c8de898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 15:39:41,331 - INFO - üìä Loading taxi data...\n",
      "2025-06-09 15:39:41,472 - INFO - ‚úÖ Loaded 55 partitions\n",
      "2025-06-09 15:39:41,472 - INFO - Columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'RateCodeID', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: 47248845 rows\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2015-01-15 19:05:39</td>\n",
       "      <td>2015-01-15 19:23:42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-73.993896</td>\n",
       "      <td>40.750111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.974785</td>\n",
       "      <td>40.750618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:53:28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-74.001648</td>\n",
       "      <td>40.724243</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.994415</td>\n",
       "      <td>40.759109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:43:41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-73.963341</td>\n",
       "      <td>40.802788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.951820</td>\n",
       "      <td>40.824413</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:35:31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>40.713818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004326</td>\n",
       "      <td>40.719986</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:52:58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-73.971176</td>\n",
       "      <td>40.762428</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004181</td>\n",
       "      <td>40.742653</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0       2.0  2015-01-15 19:05:39   2015-01-15 19:23:42              1.0   \n",
       "1       1.0  2015-01-10 20:33:38   2015-01-10 20:53:28              1.0   \n",
       "2       1.0  2015-01-10 20:33:38   2015-01-10 20:43:41              1.0   \n",
       "3       1.0  2015-01-10 20:33:39   2015-01-10 20:35:31              1.0   \n",
       "4       1.0  2015-01-10 20:33:39   2015-01-10 20:52:58              1.0   \n",
       "\n",
       "   trip_distance  pickup_longitude  pickup_latitude  RateCodeID  \\\n",
       "0           1.59        -73.993896        40.750111         1.0   \n",
       "1           3.30        -74.001648        40.724243         1.0   \n",
       "2           1.80        -73.963341        40.802788         1.0   \n",
       "3           0.50        -74.009087        40.713818         1.0   \n",
       "4           3.00        -73.971176        40.762428         1.0   \n",
       "\n",
       "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "0                  N         -73.974785         40.750618           1.0   \n",
       "1                  N         -73.994415         40.759109           1.0   \n",
       "2                  N         -73.951820         40.824413           2.0   \n",
       "3                  N         -74.004326         40.719986           2.0   \n",
       "4                  N         -74.004181         40.742653           2.0   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0         12.0    1.0      0.5        3.25           0.0   \n",
       "1         14.5    0.5      0.5        2.00           0.0   \n",
       "2          9.5    0.5      0.5        0.00           0.0   \n",
       "3          3.5    0.5      0.5        0.00           0.0   \n",
       "4         15.0    0.5      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  \n",
       "0                    0.3         17.05  \n",
       "1                    0.3         17.80  \n",
       "2                    0.3         10.80  \n",
       "3                    0.3          4.80  \n",
       "4                    0.3         16.30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and preprocess taxi data\n",
    "def load_taxi_data():\n",
    "    \"\"\"Load taxi data with proper preprocessing.\"\"\"\n",
    "    logger.info(\"üìä Loading taxi data...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data with specified dtypes\n",
    "        dtypes = processor.get_taxi_dtypes()\n",
    "        \n",
    "        ddf = dd.read_csv(\n",
    "            config.RAW_TAXI_DATA_PATTERN,\n",
    "            blocksize='128MB',\n",
    "            dtype=dtypes,\n",
    "            assume_missing=True,\n",
    "            on_bad_lines='skip'  # replaces `error_bad_lines=False` in pandas ‚â•1.3.0\n",
    "        )\n",
    "\n",
    "            \n",
    "        # Convert datetime columns\n",
    "        date_cols = [config.PICKUP_DATETIME_COL, config.DROPOFF_DATETIME_COL]\n",
    "        for col in date_cols:\n",
    "            if col in ddf.columns:\n",
    "                ddf[col] = dd.to_datetime(ddf[col], errors='coerce')\n",
    "        \n",
    "        # Ensure lat/lon columns are numeric\n",
    "        coord_cols = [config.PICKUP_LAT_COL, config.PICKUP_LON_COL, \n",
    "                     config.DROPOFF_LAT_COL, config.DROPOFF_LON_COL]\n",
    "        \n",
    "        for col in coord_cols:\n",
    "            if col in ddf.columns:\n",
    "                if not pd.api.types.is_numeric_dtype(ddf[col].dtype):\n",
    "                    ddf[col] = dd.to_numeric(ddf[col], errors='coerce')\n",
    "        \n",
    "        logger.info(f\"‚úÖ Loaded {ddf.npartitions} partitions\")\n",
    "        logger.info(f\"Columns: {list(ddf.columns)}\")\n",
    "        \n",
    "        return ddf\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load taxi data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the data\n",
    "raw_taxi_ddf = load_taxi_data()\n",
    "print(f\"Data shape: {raw_taxi_ddf.map_partitions(len).sum().compute()} rows\")\n",
    "print(f\"Sample data:\")\n",
    "display(raw_taxi_ddf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ad3019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 15:41:11,792 - INFO - üî∑ Adding H3 zones...\n",
      "2025-06-09 15:41:11,805 - INFO - Adding H3 resolution 7...\n",
      "2025-06-09 15:41:11,840 - INFO - Adding H3 resolution 8...\n",
      "2025-06-09 15:41:11,846 - INFO - Adding H3 resolution 9...\n",
      "2025-06-09 15:41:11,892 - INFO - ‚úÖ H3 zones and temporal features added\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample with H3 zones:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_h3_r8</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.750111</td>\n",
       "      <td>-73.993896</td>\n",
       "      <td>None</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.724243</td>\n",
       "      <td>-74.001648</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.802788</td>\n",
       "      <td>-73.963341</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.713818</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.762428</td>\n",
       "      <td>-73.971176</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_latitude  pickup_longitude pickup_h3_r8  pickup_hour  \\\n",
       "0        40.750111        -73.993896         None           19   \n",
       "1        40.724243        -74.001648         None           20   \n",
       "2        40.802788        -73.963341         None           20   \n",
       "3        40.713818        -74.009087         None           20   \n",
       "4        40.762428        -73.971176         None           20   \n",
       "\n",
       "   pickup_day_of_week  \n",
       "0                   3  \n",
       "1                   5  \n",
       "2                   5  \n",
       "3                   5  \n",
       "4                   5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add H3 hexagon zones for different resolutions\n",
    "def add_h3_zones(ddf):\n",
    "    \"\"\"Add H3 hexagon zones at multiple resolutions.\"\"\"\n",
    "    logger.info(\"üî∑ Adding H3 zones...\")\n",
    "    \n",
    "    processed_ddf = ddf.copy()\n",
    "    \n",
    "    # Check if coordinate columns exist\n",
    "    if not all(col in ddf.columns for col in [config.PICKUP_LAT_COL, config.PICKUP_LON_COL]):\n",
    "        logger.warning(\"‚ùå Coordinate columns not found\")\n",
    "        return processed_ddf\n",
    "    \n",
    "    # Add H3 zones for each resolution\n",
    "    for resolution in config.H3_RESOLUTIONS:\n",
    "        h3_col = f'pickup_h3_r{resolution}'\n",
    "        logger.info(f\"Adding H3 resolution {resolution}...\")\n",
    "        \n",
    "        # Create metadata for new column\n",
    "        meta = processed_ddf._meta.copy()\n",
    "        meta[h3_col] = 'object'\n",
    "        \n",
    "        # Apply H3 conversion\n",
    "        processed_ddf = processed_ddf.map_partitions(\n",
    "            processor.apply_h3_to_partition,\n",
    "            lat_col=config.PICKUP_LAT_COL,\n",
    "            lon_col=config.PICKUP_LON_COL,\n",
    "            resolution=resolution,\n",
    "            h3_col_name=h3_col,\n",
    "            meta=meta\n",
    "        )\n",
    "    \n",
    "    # Add temporal features\n",
    "    if config.PICKUP_DATETIME_COL in processed_ddf.columns:\n",
    "        processed_ddf['pickup_hour'] = processed_ddf[config.PICKUP_DATETIME_COL].dt.hour\n",
    "        processed_ddf['pickup_day_of_week'] = processed_ddf[config.PICKUP_DATETIME_COL].dt.dayofweek\n",
    "        processed_ddf['pickup_month'] = processed_ddf[config.PICKUP_DATETIME_COL].dt.month\n",
    "        processed_ddf['pickup_date'] = processed_ddf[config.PICKUP_DATETIME_COL].dt.date\n",
    "    \n",
    "    logger.info(\"‚úÖ H3 zones and temporal features added\")\n",
    "    return processed_ddf\n",
    "\n",
    "# Process the data\n",
    "processed_ddf = add_h3_zones(raw_taxi_ddf)\n",
    "\n",
    "# Show sample with H3 zones\n",
    "h3_sample = processed_ddf[[\n",
    "    config.PICKUP_LAT_COL, config.PICKUP_LON_COL,\n",
    "    f'pickup_h3_r{config.H3_MAIN_RESOLUTION}',\n",
    "    'pickup_hour', 'pickup_day_of_week'\n",
    "]].head()\n",
    "\n",
    "print(\"Sample with H3 zones:\")\n",
    "display(h3_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ea8279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 15:41:51,420 - INFO - ‚úÖ Loaded 263 taxi zones\n",
      "2025-06-09 15:41:51,423 - WARNING - ‚ö†Ô∏è Boroughs shapefile not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi_zones: 263 features\n",
      "  Columns: ['OBJECTID', 'Shape_Leng', 'Shape_Area', 'zone', 'LocationID', 'borough', 'geometry']\n",
      "  CRS: EPSG:2263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load geospatial reference data\n",
    "def load_geospatial_data():\n",
    "    \"\"\"Load NYC geospatial reference data.\"\"\"\n",
    "    geodata = {}\n",
    "    \n",
    "    # Try to load taxi zones\n",
    "    try:\n",
    "        if os.path.exists(config.TAXI_ZONES_SHAPEFILE):\n",
    "            geodata['taxi_zones'] = gpd.read_file(config.TAXI_ZONES_SHAPEFILE)\n",
    "            logger.info(f\"‚úÖ Loaded {len(geodata['taxi_zones'])} taxi zones\")\n",
    "        else:\n",
    "            logger.warning(\"‚ö†Ô∏è Taxi zones shapefile not found\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load taxi zones: {e}\")\n",
    "    \n",
    "    # Try to load boroughs\n",
    "    try:\n",
    "        if os.path.exists(config.NYC_BOROUGHS_SHAPEFILE):\n",
    "            geodata['boroughs'] = gpd.read_file(config.NYC_BOROUGHS_SHAPEFILE)\n",
    "            logger.info(f\"‚úÖ Loaded {len(geodata['boroughs'])} boroughs\")\n",
    "        else:\n",
    "            logger.warning(\"‚ö†Ô∏è Boroughs shapefile not found\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load boroughs: {e}\")\n",
    "    \n",
    "    # Create NYC boundary if no shapefiles available\n",
    "    if not geodata:\n",
    "        logger.info(\"üìç Creating NYC boundary polygon\")\n",
    "        bounds = config.NYC_BOUNDS\n",
    "        nyc_polygon = Polygon([\n",
    "            (bounds['lon_min'], bounds['lat_min']),\n",
    "            (bounds['lon_max'], bounds['lat_min']),\n",
    "            (bounds['lon_max'], bounds['lat_max']),\n",
    "            (bounds['lon_min'], bounds['lat_max'])\n",
    "        ])\n",
    "        geodata['nyc_boundary'] = gpd.GeoDataFrame(\n",
    "            {'name': ['NYC']}, \n",
    "            geometry=[nyc_polygon], \n",
    "            crs='EPSG:4326'\n",
    "        )\n",
    "    \n",
    "    return geodata\n",
    "\n",
    "# Load geospatial data\n",
    "geo_data = load_geospatial_data()\n",
    "\n",
    "# Display available geospatial data\n",
    "for key, gdf in geo_data.items():\n",
    "    print(f\"{key}: {len(gdf)} features\")\n",
    "    if len(gdf) > 0:\n",
    "        print(f\"  Columns: {list(gdf.columns)}\")\n",
    "        print(f\"  CRS: {gdf.crs}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32f0dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 15:41:51,442 - INFO - üîç Assessing data quality...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The columns in the computed data do not match the columns in the provided metadata.\n  Extra:   ['RatecodeID']\n  Missing: ['RateCodeID']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 43\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_rows\u001b[39m\u001b[38;5;124m'\u001b[39m: total_rows,\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquality_df\u001b[39m\u001b[38;5;124m'\u001b[39m: quality_df,\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoord_stats\u001b[39m\u001b[38;5;124m'\u001b[39m: coord_stats\n\u001b[0;32m     40\u001b[0m     }\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Assess data quality\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m quality_assessment \u001b[38;5;241m=\u001b[39m \u001b[43massess_data_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_ddf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä Data Quality Assessment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquality_assessment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_rows\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36massess_data_quality\u001b[1;34m(ddf)\u001b[0m\n\u001b[0;32m      4\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Assessing data quality...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Basic statistics\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mddf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_rows\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Missing values analysis\u001b[39;00m\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:391\u001b[0m, in \u001b[0;36mFrameBase.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\base.py:373\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\base.py:681\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[0;32m    679\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(flatten(expr\u001b[38;5;241m.\u001b[39m__dask_keys__()))\n\u001b[1;32m--> 681\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(expr, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\dataframe\\utils.py:400\u001b[0m, in \u001b[0;36mcheck_matching_columns\u001b[1;34m()\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     extra_info \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrder of columns does not match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mActual:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe columns in the computed data do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the columns in the provided metadata.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: The columns in the computed data do not match the columns in the provided metadata.\n  Extra:   ['RatecodeID']\n  Missing: ['RateCodeID']"
     ]
    }
   ],
   "source": [
    "# Data Quality Assessment\n",
    "def assess_data_quality(ddf):\n",
    "    \"\"\"Comprehensive data quality assessment.\"\"\"\n",
    "    logger.info(\"üîç Assessing data quality...\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = len(ddf)\n",
    "    logger.info(f\"Total rows: {total_rows:,}\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    null_counts = ddf.isnull().sum().compute()\n",
    "    null_percentages = (null_counts / total_rows * 100).round(2)\n",
    "    \n",
    "    quality_df = pd.DataFrame({\n",
    "        'Column': null_counts.index,\n",
    "        'Null_Count': null_counts.values,\n",
    "        'Null_Percentage': null_percentages.values\n",
    "    })\n",
    "    quality_df = quality_df[quality_df['Null_Count'] > 0].sort_values('Null_Percentage', ascending=False)\n",
    "    \n",
    "    # Coordinate validity check\n",
    "    coord_stats = {}\n",
    "    coord_cols = [config.PICKUP_LAT_COL, config.PICKUP_LON_COL]\n",
    "    \n",
    "    for col in coord_cols:\n",
    "        if col in ddf.columns:\n",
    "            valid_coords = ddf[col].between(\n",
    "                config.NYC_BOUNDS['lat_min'] if 'lat' in col else config.NYC_BOUNDS['lon_min'],\n",
    "                config.NYC_BOUNDS['lat_max'] if 'lat' in col else config.NYC_BOUNDS['lon_max']\n",
    "            ).sum().compute()\n",
    "            coord_stats[col] = {\n",
    "                'valid_count': valid_coords,\n",
    "                'valid_percentage': (valid_coords / total_rows * 100).round(2)\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'quality_df': quality_df,\n",
    "        'coord_stats': coord_stats\n",
    "    }\n",
    "\n",
    "# Assess data quality\n",
    "quality_assessment = assess_data_quality(processed_ddf)\n",
    "\n",
    "print(f\"üìä Data Quality Assessment\")\n",
    "print(f\"Total rows: {quality_assessment['total_rows']:,}\")\n",
    "print(\"\\nüîç Missing Values:\")\n",
    "display(quality_assessment['quality_df'])\n",
    "\n",
    "print(\"\\nüìç Coordinate Validity:\")\n",
    "for col, stats in quality_assessment['coord_stats'].items():\n",
    "    print(f\"{col}: {stats['valid_count']:,} valid ({stats['valid_percentage']:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
