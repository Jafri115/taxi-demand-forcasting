{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187bbdaa",
   "metadata": {},
   "source": [
    "# Enhanced Geospatial EDA for NYC Taxi Data\n",
    "## Using Dask, GeoPandas, H3, and Advanced Visualization\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on NYC taxi data with focus on geospatial patterns, temporal trends, and advanced visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3874ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core data processing\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geospatial libraries\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import wkt\n",
    "import contextily as ctx\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster, FastMarkerCluster\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Utilities\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5de09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "class Config:\n",
    "    # Data paths\n",
    "    RAW_TAXI_DATA_PATTERN = \"../data/yellow_tripdata_*.csv\" # Adjust if your data is elsewhere\n",
    "    TAXI_ZONES_SHAPEFILE = \"../data/taxi_zones/taxi_zones.shp\" # Adjust if your data is elsewhere\n",
    "    NYC_BOROUGHS_SHAPEFILE = \"../data/boroughs/boroughs.shp\" # Adjust if your data is elsewhere\n",
    "    \n",
    "    # Create dummy data and directories if they don't exist for notebook execution\n",
    "    if not os.path.exists(\"../data\"): os.makedirs(\"../data\")\n",
    "    if not os.path.exists(\"../data/taxi_zones\"): os.makedirs(\"../data/taxi_zones\")\n",
    "    if not os.path.exists(\"../data/boroughs\"): os.makedirs(\"../data/boroughs\")\n",
    "    \n",
    "    # Create a dummy CSV file if none match the pattern\n",
    "    import glob\n",
    "    if not glob.glob(RAW_TAXI_DATA_PATTERN):\n",
    "        print(\"Creating dummy taxi data for demonstration...\")\n",
    "        dummy_df = pd.DataFrame({\n",
    "            'tpep_pickup_datetime': pd.to_datetime(['2023-01-01 10:00:00', '2023-01-01 10:15:00']),\n",
    "            'tpep_dropoff_datetime': pd.to_datetime(['2023-01-01 10:10:00', '2023-01-01 10:30:00']),\n",
    "            'pickup_longitude': [-73.985130, -73.990000],\n",
    "            'pickup_latitude': [40.758896, 40.750000],\n",
    "            'dropoff_longitude': [-73.995000, -73.980000],\n",
    "            'dropoff_latitude': [40.740000, 40.760000],\n",
    "            'passenger_count': [1.0, 2.0],\n",
    "            'trip_distance': [1.5, 2.0],\n",
    "            'fare_amount': [10.0, 12.5],\n",
    "            'tip_amount': [2.0, 2.5],\n",
    "            'total_amount': [15.0, 18.0],\n",
    "            'RatecodeID': [1.0, 1.0],\n",
    "            'VendorID': [1.0, 2.0],\n",
    "            'payment_type': [1.0, 1.0] \n",
    "        })\n",
    "        dummy_df.to_csv(\"../data/yellow_tripdata_dummy.csv\", index=False)\n",
    "    \n",
    "    # Column names\n",
    "    PICKUP_DATETIME_COL = 'tpep_pickup_datetime'\n",
    "    DROPOFF_DATETIME_COL = 'tpep_dropoff_datetime'\n",
    "    PICKUP_LAT_COL = 'pickup_latitude'\n",
    "    PICKUP_LON_COL = 'pickup_longitude'\n",
    "    DROPOFF_LAT_COL = 'dropoff_latitude'\n",
    "    DROPOFF_LON_COL = 'dropoff_longitude'\n",
    "    \n",
    "    # H3 configuration\n",
    "    H3_RESOLUTIONS = [7, 8, 9]  # Multiple resolutions for different analyses\n",
    "    H3_MAIN_RESOLUTION = 8\n",
    "    \n",
    "    # NYC boundaries (approximate)\n",
    "    NYC_BOUNDS = {\n",
    "        'lat_min': 40.4774, 'lat_max': 40.9176,\n",
    "        'lon_min': -74.2591, 'lon_max': -73.7004\n",
    "    }\n",
    "    \n",
    "    # Dask configuration\n",
    "    DASK_WORKERS = 2 # Reduced for local testing, adjust as needed\n",
    "    DASK_THREADS_PER_WORKER = 2\n",
    "    DASK_MEMORY_LIMIT = '2GB' # Reduced for local testing\n",
    "    \n",
    "    # Sampling rates for different analyses\n",
    "    SAMPLE_RATES = {\n",
    "        'visualization': 0.01,  # 1% for general viz\n",
    "        'clustering': 0.005, # 0.5% for DBSCAN\n",
    "        'heatmap': 0.1 # 10% for Folium Heatmap (can be higher for small datasets)\n",
    "    }\n",
    "\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12dbe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 19:50:31,278 - INFO - üöÄ Dask Client initialized: http://127.0.0.1:53763/status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Dashboard: http://127.0.0.1:53763/status\n"
     ]
    }
   ],
   "source": [
    "# Setup Dask Client with optimized configuration\n",
    "def setup_dask_client():\n",
    "    \"\"\"Initialize Dask client with proper cleanup.\"\"\"\n",
    "    global client, cluster # Make them global to be accessible for cleanup\n",
    "    try:\n",
    "        # Clean up existing clients\n",
    "        if 'client' in globals() and client:\n",
    "            client.close()\n",
    "        if 'cluster' in globals() and cluster:\n",
    "            cluster.close()\n",
    "    except (NameError, Exception) as e:\n",
    "        logger.debug(f\"No existing client to close: {e}\")\n",
    "    \n",
    "    try:\n",
    "        cluster = LocalCluster(\n",
    "            n_workers=config.DASK_WORKERS,\n",
    "            threads_per_worker=config.DASK_THREADS_PER_WORKER,\n",
    "            memory_limit=config.DASK_MEMORY_LIMIT,\n",
    "            dashboard_address=':8787', # Use a fixed port if possible, or None for auto\n",
    "            silence_logs=logging.ERROR # Silence Dask's own INFO logs unless error\n",
    "        )\n",
    "        client = Client(cluster)\n",
    "        logger.info(f\"üöÄ Dask Client initialized: {client.dashboard_link}\")\n",
    "        return client, cluster\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to initialize Dask client: {e}\")\n",
    "        # Fallback to a default client if LocalCluster setup fails\n",
    "        try:\n",
    "            client = Client()\n",
    "            cluster = None # No explicit cluster object in this case\n",
    "            logger.info(f\"üöÄ Dask Client initialized with defaults: {client.dashboard_link}\")\n",
    "            return client, cluster\n",
    "        except Exception as e_fallback:\n",
    "            logger.error(f\"‚ùå Failed to initialize Dask with defaults: {e_fallback}\")\n",
    "            return None, None\n",
    "\n",
    "client, cluster = setup_dask_client()\n",
    "if client:\n",
    "    print(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "else:\n",
    "    print(\"Dask client not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394d1b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data processing utilities loaded\n"
     ]
    }
   ],
   "source": [
    "# Utility functions for data processing\n",
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def get_taxi_dtypes():\n",
    "        \"\"\"Define data types for taxi data to ensure proper loading.\"\"\"\n",
    "        return {\n",
    "            'VendorID': 'float64',\n",
    "            'passenger_count': 'float64',\n",
    "            'trip_distance': 'float64',\n",
    "            'RatecodeID': 'float64',\n",
    "            'store_and_fwd_flag': 'object',\n",
    "            config.PICKUP_LON_COL: 'float64',\n",
    "            config.PICKUP_LAT_COL: 'float64',\n",
    "            config.DROPOFF_LON_COL: 'float64',\n",
    "            config.DROPOFF_LAT_COL: 'float64',\n",
    "            'payment_type': 'float64',\n",
    "            'fare_amount': 'float64',\n",
    "            'extra': 'float64',\n",
    "            'mta_tax': 'float64',\n",
    "            'tip_amount': 'float64',\n",
    "            'tolls_amount': 'float64',\n",
    "            'improvement_surcharge': 'float64',\n",
    "            'total_amount': 'float64',\n",
    "            'congestion_surcharge': 'float64',\n",
    "            'airport_fee': 'float64' # Might not be in older data\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_nyc_bounds(df, lat_col, lon_col):\n",
    "        \"\"\"Filter data to NYC boundaries.\"\"\"\n",
    "        return df[\n",
    "            (df[lat_col].between(config.NYC_BOUNDS['lat_min'], config.NYC_BOUNDS['lat_max'])) &\n",
    "            (df[lon_col].between(config.NYC_BOUNDS['lon_min'], config.NYC_BOUNDS['lon_max']))\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_h3_convert(lat, lon, resolution):\n",
    "        \"\"\"Safely convert lat/lon to H3 hex.\"\"\"\n",
    "        try:\n",
    "            if pd.isna(lat) or pd.isna(lon):\n",
    "                return None\n",
    "            # Validate coordinates before passing to h3\n",
    "            if not (-90 <= float(lat) <= 90 and -180 <= float(lon) <= 180):\n",
    "                return None\n",
    "            return h3.geo_to_h3(float(lat), float(lon), int(resolution))\n",
    "        except (ValueError, TypeError, Exception): # Catch h3.H3ValueError and other conversion issues\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_h3_to_partition(df_partition, lat_col, lon_col, resolution, h3_col_name):\n",
    "        \"\"\"Apply H3 conversion to a Dask partition.\"\"\"\n",
    "        if lat_col not in df_partition.columns or lon_col not in df_partition.columns:\n",
    "            df_partition[h3_col_name] = pd.Series([None] * len(df_partition), index=df_partition.index, dtype='object')\n",
    "            return df_partition\n",
    "        \n",
    "        # Ensure lat/lon are numeric before applying\n",
    "        df_partition[lat_col] = pd.to_numeric(df_partition[lat_col], errors='coerce')\n",
    "        df_partition[lon_col] = pd.to_numeric(df_partition[lon_col], errors='coerce')\n",
    "\n",
    "        df_partition[h3_col_name] = df_partition.apply(\n",
    "            lambda row: DataProcessor.safe_h3_convert(row[lat_col], row[lon_col], resolution),\n",
    "            axis=1\n",
    "        )\n",
    "        return df_partition\n",
    "\n",
    "processor = DataProcessor()\n",
    "print(\"‚úÖ Data processing utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb898b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 19:50:42,776 - INFO - üìä Loading taxi data...\n",
      "2025-06-04 19:50:42,945 - INFO - ‚úÖ Loaded 113 partitions\n",
      "2025-06-04 19:50:42,946 - INFO - Columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'RateCodeID', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'RatecodeID', 'congestion_surcharge', 'airport_fee']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: 47248845 rows\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2015-01-15 19:05:39</td>\n",
       "      <td>2015-01-15 19:23:42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-73.993896</td>\n",
       "      <td>40.750111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.974785</td>\n",
       "      <td>40.750618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:53:28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-74.001648</td>\n",
       "      <td>40.724243</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.994415</td>\n",
       "      <td>40.759109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:43:41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-73.963341</td>\n",
       "      <td>40.802788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.951820</td>\n",
       "      <td>40.824413</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:35:31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>40.713818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004326</td>\n",
       "      <td>40.719986</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:52:58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-73.971176</td>\n",
       "      <td>40.762428</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004181</td>\n",
       "      <td>40.742653</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0       2.0  2015-01-15 19:05:39   2015-01-15 19:23:42              1.0   \n",
       "1       1.0  2015-01-10 20:33:38   2015-01-10 20:53:28              1.0   \n",
       "2       1.0  2015-01-10 20:33:38   2015-01-10 20:43:41              1.0   \n",
       "3       1.0  2015-01-10 20:33:39   2015-01-10 20:35:31              1.0   \n",
       "4       1.0  2015-01-10 20:33:39   2015-01-10 20:52:58              1.0   \n",
       "\n",
       "   trip_distance  pickup_longitude  pickup_latitude  RateCodeID  \\\n",
       "0           1.59        -73.993896        40.750111         1.0   \n",
       "1           3.30        -74.001648        40.724243         1.0   \n",
       "2           1.80        -73.963341        40.802788         1.0   \n",
       "3           0.50        -74.009087        40.713818         1.0   \n",
       "4           3.00        -73.971176        40.762428         1.0   \n",
       "\n",
       "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "0                  N         -73.974785         40.750618           1.0   \n",
       "1                  N         -73.994415         40.759109           1.0   \n",
       "2                  N         -73.951820         40.824413           2.0   \n",
       "3                  N         -74.004326         40.719986           2.0   \n",
       "4                  N         -74.004181         40.742653           2.0   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0         12.0    1.0      0.5        3.25           0.0   \n",
       "1         14.5    0.5      0.5        2.00           0.0   \n",
       "2          9.5    0.5      0.5        0.00           0.0   \n",
       "3          3.5    0.5      0.5        0.00           0.0   \n",
       "4         15.0    0.5      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  RatecodeID  congestion_surcharge  \\\n",
       "0                    0.3         17.05         NaN                   NaN   \n",
       "1                    0.3         17.80         NaN                   NaN   \n",
       "2                    0.3         10.80         NaN                   NaN   \n",
       "3                    0.3          4.80         NaN                   NaN   \n",
       "4                    0.3         16.30         NaN                   NaN   \n",
       "\n",
       "   airport_fee  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and preprocess taxi data\n",
    "def load_taxi_data():\n",
    "    \"\"\"Load taxi data with proper preprocessing.\"\"\"\n",
    "    logger.info(\"üìä Loading taxi data...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data with specified dtypes\n",
    "        dtypes = processor.get_taxi_dtypes()\n",
    "        \n",
    "        # Use all columns from dtypes, but only if they exist in CSV, handle missing ones\n",
    "        # Dask's read_csv will try to infer types if not specified, or error if dtype mismatch\n",
    "        # We provide 'assume_missing=True' to handle columns that might not be in all files\n",
    "        ddf = dd.read_csv(\n",
    "            config.RAW_TAXI_DATA_PATTERN,\n",
    "            blocksize='64MB', # Smaller blocksize for potentially smaller files/memory\n",
    "            dtype=dtypes,\n",
    "            assume_missing=True, # Handles columns missing in some files\n",
    "            # Following parameters help with parsing robustness\n",
    "            # na_filter=False, # If you want to treat empty strings as strings not NaN, not typical for numeric data\n",
    "            # skipinitialspace=True, # good for some CSVs\n",
    "            # on_bad_lines='warn' # or 'skip' or a callable\n",
    "        )\n",
    "        \n",
    "        # Ensure essential columns exist, fill with NaN if not (due to assume_missing)\n",
    "        # This is more for Dask where a column might be entirely missing from a partition's meta\n",
    "        # but assume_missing in read_csv should largely handle this.\n",
    "        for col, dtype_str in dtypes.items():\n",
    "            if col not in ddf.columns:\n",
    "                if 'datetime' in col.lower(): \n",
    "                    ddf[col] = pd.NaT # For datetime\n",
    "                elif dtype_str.startswith('float') or dtype_str.startswith('int'):\n",
    "                    ddf[col] = np.nan # For numeric\n",
    "                else:\n",
    "                    ddf[col] = pd.NA # For object/string using pandas NA\n",
    "        \n",
    "        # Convert datetime columns\n",
    "        date_cols = [config.PICKUP_DATETIME_COL, config.DROPOFF_DATETIME_COL]\n",
    "        for col in date_cols:\n",
    "            if col in ddf.columns:\n",
    "                ddf[col] = dd.to_datetime(ddf[col], errors='coerce')\n",
    "        \n",
    "        # Ensure lat/lon columns are numeric\n",
    "        coord_cols = [config.PICKUP_LAT_COL, config.PICKUP_LON_COL, \n",
    "                     config.DROPOFF_LAT_COL, config.DROPOFF_LON_COL]\n",
    "        \n",
    "        for col in coord_cols:\n",
    "            if col in ddf.columns:\n",
    "                # Check if conversion is needed to avoid re-converting if already numeric\n",
    "                if not pd.api.types.is_numeric_dtype(ddf[col].dtype):\n",
    "                    ddf[col] = dd.to_numeric(ddf[col], errors='coerce')\n",
    "        \n",
    "        logger.info(f\"‚úÖ Loaded {ddf.npartitions} partitions\")\n",
    "        logger.info(f\"Columns: {list(ddf.columns)}\")\n",
    "        \n",
    "        # Persist if client is available to speed up subsequent operations\n",
    "        # if client: \n",
    "        #    logger.info(\"Persisting raw_taxi_ddf to memory...\")\n",
    "        #    ddf = ddf.persist()\n",
    "            \n",
    "        return ddf\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load taxi data: {e}\")\n",
    "        # Create a minimal Dask DataFrame for the rest of the notebook to run if loading fails\n",
    "        # This is useful for testing the notebook structure without full data\n",
    "        logger.warning(\"Creating a minimal Dask DataFrame to proceed.\")\n",
    "        dummy_pdf = pd.DataFrame({\n",
    "            config.PICKUP_DATETIME_COL: pd.to_datetime(['2023-01-01 10:00:00']),\n",
    "            config.DROPOFF_DATETIME_COL: pd.to_datetime(['2023-01-01 10:10:00']),\n",
    "            config.PICKUP_LAT_COL: [40.758896],\n",
    "            config.PICKUP_LON_COL: [-73.985130],\n",
    "            config.DROPOFF_LAT_COL: [40.740000],\n",
    "            config.DROPOFF_LON_COL: [-73.995000],\n",
    "            'passenger_count': [1.0],\n",
    "            'trip_distance': [1.5]\n",
    "        })\n",
    "        # Ensure all expected columns from dtypes are present\n",
    "        expected_cols = processor.get_taxi_dtypes()\n",
    "        for col_name in expected_cols.keys():\n",
    "            if col_name not in dummy_pdf.columns:\n",
    "                if 'datetime' in col_name: dummy_pdf[col_name] = pd.NaT\n",
    "                elif expected_cols[col_name].startswith('float'): dummy_pdf[col_name] = np.nan\n",
    "                else: dummy_pdf[col_name] = None \n",
    "        return dd.from_pandas(dummy_pdf, npartitions=1)\n",
    "\n",
    "# Load the data\n",
    "raw_taxi_ddf = load_taxi_data()\n",
    "if raw_taxi_ddf is not None:\n",
    "    computed_length = raw_taxi_ddf.map_partitions(len).sum().compute()\n",
    "    print(f\"Data shape: {computed_length} rows\")\n",
    "    print(f\"Sample data:\")\n",
    "    display(raw_taxi_ddf.head())\n",
    "else:\n",
    "    print(\"Taxi data loading failed, proceeding with minimal/no data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "780d3cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 19:52:38,117 - INFO - üî∑ Adding H3 zones...\n",
      "2025-06-04 19:52:38,118 - INFO - Adding H3 resolution 7 as column 'pickup_h3_r7'...\n",
      "2025-06-04 19:52:38,122 - INFO - Adding H3 resolution 8 as column 'pickup_h3_r8'...\n",
      "2025-06-04 19:52:38,124 - INFO - Adding H3 resolution 9 as column 'pickup_h3_r9'...\n",
      "2025-06-04 19:52:38,127 - INFO - Adding temporal features (hour, day_of_week, month, date)...\n",
      "2025-06-04 19:52:38,140 - INFO - ‚úÖ H3 zones and temporal features processed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample with H3 zones and temporal features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_h3_r7</th>\n",
       "      <th>pickup_h3_r8</th>\n",
       "      <th>pickup_h3_r9</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.750111</td>\n",
       "      <td>-73.993896</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.724243</td>\n",
       "      <td>-74.001648</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.802788</td>\n",
       "      <td>-73.963341</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.713818</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.762428</td>\n",
       "      <td>-73.971176</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_latitude  pickup_longitude pickup_h3_r7 pickup_h3_r8 pickup_h3_r9  \\\n",
       "0        40.750111        -73.993896         None         None         None   \n",
       "1        40.724243        -74.001648         None         None         None   \n",
       "2        40.802788        -73.963341         None         None         None   \n",
       "3        40.713818        -74.009087         None         None         None   \n",
       "4        40.762428        -73.971176         None         None         None   \n",
       "\n",
       "   pickup_hour  pickup_day_of_week  \n",
       "0           19                   3  \n",
       "1           20                   5  \n",
       "2           20                   5  \n",
       "3           20                   5  \n",
       "4           20                   5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add H3 hexagon zones for different resolutions\n",
    "def add_h3_zones(ddf):\n",
    "    \"\"\"Add H3 hexagon zones at multiple resolutions.\"\"\"\n",
    "    logger.info(\"üî∑ Adding H3 zones...\")\n",
    "    \n",
    "    processed_ddf = ddf.copy()\n",
    "    \n",
    "    # Check if coordinate columns exist\n",
    "    if not all(col in ddf.columns for col in [config.PICKUP_LAT_COL, config.PICKUP_LON_COL]):\n",
    "        logger.warning(f\"‚ùå Coordinate columns ({config.PICKUP_LAT_COL}, {config.PICKUP_LON_COL}) not found. Skipping H3 zone addition.\")\n",
    "        return processed_ddf\n",
    "    \n",
    "    # Add H3 zones for each resolution\n",
    "    for resolution in config.H3_RESOLUTIONS:\n",
    "        h3_col = f'pickup_h3_r{resolution}'\n",
    "        logger.info(f\"Adding H3 resolution {resolution} as column '{h3_col}'...\")\n",
    "        \n",
    "        # Define metadata for the new column explicitly\n",
    "        meta_with_h3 = processed_ddf._meta.copy()\n",
    "        meta_with_h3[h3_col] = 'object' \n",
    "        \n",
    "        # Apply H3 conversion\n",
    "        processed_ddf = processed_ddf.map_partitions(\n",
    "            processor.apply_h3_to_partition,\n",
    "            lat_col=config.PICKUP_LAT_COL,\n",
    "            lon_col=config.PICKUP_LON_COL,\n",
    "            resolution=resolution,\n",
    "            h3_col_name=h3_col,\n",
    "            meta=meta_with_h3 # Pass the new meta\n",
    "        )\n",
    "    \n",
    "    # Add temporal features\n",
    "    if config.PICKUP_DATETIME_COL in processed_ddf.columns and pd.api.types.is_datetime64_any_dtype(processed_ddf[config.PICKUP_DATETIME_COL].dtype):\n",
    "        logger.info(\"Adding temporal features (hour, day_of_week, month, date)...\")\n",
    "        processed_ddf['pickup_hour'] = processed_ddf[config.PICKUP_DATETIME_COL].dt.hour\n",
    "        processed_ddf['pickup_day_of_week'] = processed_ddf[config.PICKUP_DATETIME_COL].dt.dayofweek\n",
    "        processed_ddf['pickup_month'] = processed_ddf[config.PICKUP_DATETIME_COL].dt.month\n",
    "        processed_ddf['pickup_date'] = processed_ddf[config.PICKUP_DATETIME_COL].dt.date\n",
    "    else:\n",
    "        logger.warning(f\"'{config.PICKUP_DATETIME_COL}' not found or not datetime. Skipping temporal feature addition.\")\n",
    "        # Add dummy columns if they don't exist to prevent errors later if code expects them\n",
    "        for col in ['pickup_hour', 'pickup_day_of_week', 'pickup_month', 'pickup_date']:\n",
    "            if col not in processed_ddf.columns:\n",
    "                 processed_ddf[col] = np.nan \n",
    "\n",
    "    logger.info(\"‚úÖ H3 zones and temporal features processed.\")\n",
    "    return processed_ddf\n",
    "\n",
    "# Process the data\n",
    "if raw_taxi_ddf is not None:\n",
    "    processed_ddf = add_h3_zones(raw_taxi_ddf)\n",
    "    \n",
    "    # Persist processed_ddf if client is available\n",
    "    # if client:\n",
    "    #    logger.info(\"Persisting processed_ddf to memory...\")\n",
    "    #    processed_ddf = processed_ddf.persist()\n",
    "    \n",
    "    # Show sample with H3 zones\n",
    "    h3_cols_to_show = [f'pickup_h3_r{res}' for res in config.H3_RESOLUTIONS if f'pickup_h3_r{res}' in processed_ddf.columns]\n",
    "    cols_to_display = [config.PICKUP_LAT_COL, config.PICKUP_LON_COL] + h3_cols_to_show + ['pickup_hour', 'pickup_day_of_week']\n",
    "    # Ensure columns exist before trying to display them\n",
    "    cols_to_display = [col for col in cols_to_display if col in processed_ddf.columns]\n",
    "    \n",
    "    if cols_to_display:\n",
    "        h3_sample = processed_ddf[cols_to_display].head()\n",
    "        print(\"Sample with H3 zones and temporal features:\")\n",
    "        display(h3_sample)\n",
    "    else:\n",
    "        print(\"Relevant columns for H3 sample not found in processed_ddf.\")\n",
    "else:\n",
    "    print(\"Skipping H3 zone addition as raw_taxi_ddf is not loaded.\")\n",
    "    processed_ddf = None # Ensure it's None if not processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d172610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 19:53:04,073 - INFO - ‚úÖ Loaded 263 taxi zones\n",
      "2025-06-04 19:53:04,081 - WARNING - ‚ö†Ô∏è Boroughs shapefile not found at ../data/boroughs/boroughs.shp\n",
      "2025-06-04 19:53:04,082 - INFO - üìç Creating NYC boundary polygon from config.NYC_BOUNDS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- taxi_zones ---\n",
      "  Features: 263\n",
      "  Columns: ['OBJECTID', 'Shape_Leng', 'Shape_Area', 'zone', 'LocationID', 'borough', 'geometry']\n",
      "  CRS: EPSG:2263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((1033269.244 172126.008, 103343...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID  Shape_Leng  Shape_Area            zone  LocationID borough  \\\n",
       "0         1    0.116357    0.000782  Newark Airport           1     EWR   \n",
       "1         2    0.433470    0.004866     Jamaica Bay           2  Queens   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((933100.918 192536.086, 933091.011 19...  \n",
       "1  MULTIPOLYGON (((1033269.244 172126.008, 103343...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- nyc_boundary ---\n",
      "  Features: 1\n",
      "  Columns: ['name', 'geometry']\n",
      "  CRS: EPSG:4326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NYC Config Boundary</td>\n",
       "      <td>POLYGON ((-74.2591 40.4774, -73.7004 40.4774, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                                           geometry\n",
       "0  NYC Config Boundary  POLYGON ((-74.2591 40.4774, -73.7004 40.4774, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load geospatial reference data\n",
    "def load_geospatial_data():\n",
    "    \"\"\"Load NYC geospatial reference data.\"\"\"\n",
    "    geodata = {}\n",
    "    \n",
    "    # Try to load taxi zones\n",
    "    try:\n",
    "        if os.path.exists(config.TAXI_ZONES_SHAPEFILE):\n",
    "            geodata['taxi_zones'] = gpd.read_file(config.TAXI_ZONES_SHAPEFILE)\n",
    "            logger.info(f\"‚úÖ Loaded {len(geodata['taxi_zones'])} taxi zones\")\n",
    "        else:\n",
    "            logger.warning(f\"‚ö†Ô∏è Taxi zones shapefile not found at {config.TAXI_ZONES_SHAPEFILE}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load taxi zones: {e}\")\n",
    "    \n",
    "    # Try to load boroughs\n",
    "    try:\n",
    "        if os.path.exists(config.NYC_BOROUGHS_SHAPEFILE):\n",
    "            geodata['boroughs'] = gpd.read_file(config.NYC_BOROUGHS_SHAPEFILE)\n",
    "            logger.info(f\"‚úÖ Loaded {len(geodata['boroughs'])} boroughs\")\n",
    "        else:\n",
    "            logger.warning(f\"‚ö†Ô∏è Boroughs shapefile not found at {config.NYC_BOROUGHS_SHAPEFILE}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load boroughs: {e}\")\n",
    "    \n",
    "    # Create NYC boundary if no shapefiles available or if specified\n",
    "    # Always create this as a fallback or general boundary\n",
    "    logger.info(\"üìç Creating NYC boundary polygon from config.NYC_BOUNDS\")\n",
    "    bounds = config.NYC_BOUNDS\n",
    "    nyc_polygon = Polygon([\n",
    "        (bounds['lon_min'], bounds['lat_min']),\n",
    "        (bounds['lon_max'], bounds['lat_min']),\n",
    "        (bounds['lon_max'], bounds['lat_max']),\n",
    "        (bounds['lon_min'], bounds['lat_max'])\n",
    "    ])\n",
    "    geodata['nyc_boundary'] = gpd.GeoDataFrame(\n",
    "        {'name': ['NYC Config Boundary']}, \n",
    "        geometry=[nyc_polygon], \n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    return geodata\n",
    "\n",
    "# Load geospatial data\n",
    "geo_data = load_geospatial_data()\n",
    "\n",
    "# Display available geospatial data\n",
    "for key, gdf in geo_data.items():\n",
    "    print(f\"--- {key} ---\")\n",
    "    if gdf is not None and not gdf.empty:\n",
    "        print(f\"  Features: {len(gdf)}\")\n",
    "        print(f\"  Columns: {list(gdf.columns)}\")\n",
    "        print(f\"  CRS: {gdf.crs}\")\n",
    "        display(gdf.head(2))\n",
    "    else:\n",
    "        print(\"  Not loaded or empty.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8e09b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The columns in the computed data do not match the columns in the provided metadata.\n  Extra:   []\n  Missing: ['RateCodeID']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocessed_ddf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:391\u001b[0m, in \u001b[0;36mFrameBase.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\base.py:373\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\base.py:681\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[0;32m    679\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(flatten(expr\u001b[38;5;241m.\u001b[39m__dask_keys__()))\n\u001b[1;32m--> 681\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(expr, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\dataframe\\utils.py:400\u001b[0m, in \u001b[0;36mcheck_matching_columns\u001b[1;34m()\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     extra_info \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrder of columns does not match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mActual:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe columns in the computed data do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the columns in the provided metadata.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: The columns in the computed data do not match the columns in the provided metadata.\n  Extra:   []\n  Missing: ['RateCodeID']"
     ]
    }
   ],
   "source": [
    "len(processed_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d35dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 20:28:15,813 - INFO - üîç Assessing data quality...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The columns in the computed data do not match the columns in the provided metadata.\n  Extra:   []\n  Missing: ['RateCodeID']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 64\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_rows\u001b[39m\u001b[38;5;124m'\u001b[39m: total_rows,\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquality_df\u001b[39m\u001b[38;5;124m'\u001b[39m: quality_df,\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoord_stats\u001b[39m\u001b[38;5;124m'\u001b[39m: coord_stats\n\u001b[0;32m     61\u001b[0m     }\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Assess data quality\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m quality_assessment \u001b[38;5;241m=\u001b[39m \u001b[43massess_data_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_ddf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä Data Quality Assessment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquality_assessment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_rows\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36massess_data_quality\u001b[1;34m(ddf)\u001b[0m\n\u001b[0;32m     12\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Assessing data quality...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Basic statistics\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mddf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# This will trigger a compute if not persisted\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_rows \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     17\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame is empty. Skipping detailed quality assessment.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:391\u001b[0m, in \u001b[0;36mFrameBase.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\base.py:373\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\base.py:681\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[0;32m    679\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(flatten(expr\u001b[38;5;241m.\u001b[39m__dask_keys__()))\n\u001b[1;32m--> 681\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(expr, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\dataframe\\utils.py:400\u001b[0m, in \u001b[0;36mcheck_matching_columns\u001b[1;34m()\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     extra_info \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrder of columns does not match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mActual:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe columns in the computed data do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the columns in the provided metadata.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: The columns in the computed data do not match the columns in the provided metadata.\n  Extra:   []\n  Missing: ['RateCodeID']"
     ]
    }
   ],
   "source": [
    "# Data Quality Assessment\n",
    "def assess_data_quality(ddf):\n",
    "    \"\"\"Comprehensive data quality assessment.\"\"\"\n",
    "    if ddf is None:\n",
    "        logger.warning(\"Input Dask DataFrame is None. Skipping data quality assessment.\")\n",
    "        return {\n",
    "            'total_rows': 0,\n",
    "            'quality_df': pd.DataFrame(columns=['Column', 'Null_Count', 'Null_Percentage']),\n",
    "            'coord_stats': {}\n",
    "        }\n",
    "        \n",
    "    logger.info(\"üîç Assessing data quality...\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = len(ddf) # This will trigger a compute if not persisted\n",
    "    if total_rows == 0:\n",
    "        logger.warning(\"DataFrame is empty. Skipping detailed quality assessment.\")\n",
    "        return {\n",
    "            'total_rows': 0,\n",
    "            'quality_df': pd.DataFrame(columns=['Column', 'Null_Count', 'Null_Percentage']),\n",
    "            'coord_stats': {}\n",
    "        }\n",
    "    logger.info(f\"Total rows (computed): {total_rows:,}\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    null_counts = ddf.isnull().sum().compute()\n",
    "    null_percentages = (null_counts / total_rows * 100).round(2)\n",
    "    \n",
    "    quality_df = pd.DataFrame({\n",
    "        'Column': null_counts.index,\n",
    "        'Null_Count': null_counts.values,\n",
    "        'Null_Percentage': null_percentages.values\n",
    "    })\n",
    "    quality_df = quality_df[quality_df['Null_Count'] > 0].sort_values('Null_Percentage', ascending=False)\n",
    "    \n",
    "    # Coordinate validity check (using pickup locations)\n",
    "    coord_stats = {}\n",
    "    coord_cols_to_check = [(config.PICKUP_LAT_COL, 'lat_min', 'lat_max'), \n",
    "                           (config.PICKUP_LON_COL, 'lon_min', 'lon_max')]\n",
    "    \n",
    "    for col, min_bound_key, max_bound_key in coord_cols_to_check:\n",
    "        if col in ddf.columns and pd.api.types.is_numeric_dtype(ddf[col].dtype):\n",
    "            # Ensure bounds are valid before using them\n",
    "            min_val = config.NYC_BOUNDS[min_bound_key]\n",
    "            max_val = config.NYC_BOUNDS[max_bound_key]\n",
    "            \n",
    "            valid_coords = ddf[col].between(min_val, max_val).sum().compute()\n",
    "            coord_stats[col] = {\n",
    "                'valid_count': valid_coords,\n",
    "                'valid_percentage': (valid_coords / total_rows * 100).round(2)\n",
    "            }\n",
    "        elif col in ddf.columns:\n",
    "             logger.warning(f\"Coordinate column {col} is not numeric, skipping validity check.\")\n",
    "        else:\n",
    "            logger.warning(f\"Coordinate column {col} not found, skipping validity check.\")\n",
    "            \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'quality_df': quality_df,\n",
    "        'coord_stats': coord_stats\n",
    "    }\n",
    "\n",
    "# Assess data quality\n",
    "quality_assessment = assess_data_quality(processed_ddf)\n",
    "\n",
    "print(f\"üìä Data Quality Assessment\")\n",
    "print(f\"Total rows: {quality_assessment['total_rows']:,}\")\n",
    "print(\"\\nüîç Missing Values:\")\n",
    "if not quality_assessment['quality_df'].empty:\n",
    "    display(quality_assessment['quality_df'])\n",
    "else:\n",
    "    print(\"No missing values found or data is empty.\")\n",
    "\n",
    "print(\"\\nüìç Coordinate Validity (Pickup points within NYC_BOUNDS):\")\n",
    "if quality_assessment['coord_stats']:\n",
    "    for col, stats in quality_assessment['coord_stats'].items():\n",
    "        print(f\"  {col}: {stats['valid_count']:,} valid ({stats['valid_percentage']:.1f}%)\")\n",
    "else:\n",
    "    print(\"Coordinate validity check not performed or no coordinate columns found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77b9645b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The columns in the computed data do not match the columns in the provided metadata.\n  Extra:   []\n  Missing: ['RateCodeID']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 63\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhourly\u001b[39m\u001b[38;5;124m'\u001b[39m: hourly_trips,\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaily\u001b[39m\u001b[38;5;124m'\u001b[39m: daily_trips,\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonthly\u001b[39m\u001b[38;5;124m'\u001b[39m: monthly_trips,\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour_dow\u001b[39m\u001b[38;5;124m'\u001b[39m: hour_dow_data\n\u001b[0;32m     60\u001b[0m     }\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Analyze temporal patterns\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m temporal_patterns \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_temporal_patterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_ddf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Create temporal visualizations\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(val\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m temporal_patterns\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m, in \u001b[0;36manalyze_temporal_patterns\u001b[1;34m(ddf)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manalyze_temporal_patterns\u001b[39m(ddf):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Analyze temporal patterns in taxi trips.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ddf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mddf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      5\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Dask DataFrame is None or empty. Skipping temporal analysis.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhourly\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mSeries(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaily\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mSeries(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      9\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonthly\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mSeries(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour_dow\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m     11\u001b[0m         }\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:391\u001b[0m, in \u001b[0;36mFrameBase.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\base.py:373\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\base.py:681\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[0;32m    679\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(flatten(expr\u001b[38;5;241m.\u001b[39m__dask_keys__()))\n\u001b[1;32m--> 681\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(expr, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "File \u001b[1;32md:\\git_projects\\taxi_demand_forecasting\\.venv\\lib\\site-packages\\dask\\dataframe\\utils.py:400\u001b[0m, in \u001b[0;36mcheck_matching_columns\u001b[1;34m()\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     extra_info \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrder of columns does not match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mActual:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe columns in the computed data do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the columns in the provided metadata.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: The columns in the computed data do not match the columns in the provided metadata.\n  Extra:   []\n  Missing: ['RateCodeID']"
     ]
    }
   ],
   "source": [
    "# Temporal Analysis\n",
    "def analyze_temporal_patterns(ddf):\n",
    "    \"\"\"Analyze temporal patterns in taxi trips.\"\"\"\n",
    "    if ddf is None or len(ddf) == 0:\n",
    "        logger.warning(\"Input Dask DataFrame is None or empty. Skipping temporal analysis.\")\n",
    "        return {\n",
    "            'hourly': pd.Series(dtype='int64'),\n",
    "            'daily': pd.Series(dtype='int64'),\n",
    "            'monthly': pd.Series(dtype='int64'),\n",
    "            'hour_dow': pd.DataFrame()\n",
    "        }\n",
    "        \n",
    "    logger.info(\"‚è∞ Analyzing temporal patterns...\")\n",
    "    \n",
    "    # Ensure temporal columns exist and are of correct type\n",
    "    required_temporal_cols = ['pickup_hour', 'pickup_day_of_week', 'pickup_month']\n",
    "    if not all(col in ddf.columns for col in required_temporal_cols):\n",
    "        logger.warning(f\"Missing one or more temporal columns: {required_temporal_cols}. Aborting temporal analysis.\")\n",
    "        return {\n",
    "            'hourly': pd.Series(dtype='int64'),\n",
    "            'daily': pd.Series(dtype='int64'),\n",
    "            'monthly': pd.Series(dtype='int64'),\n",
    "            'hour_dow': pd.DataFrame()\n",
    "        }\n",
    "\n",
    "    # Filter valid datetime data (implicitly done by feature engineering, but good to be aware)\n",
    "    # valid_datetime_ddf = ddf.dropna(subset=[config.PICKUP_DATETIME_COL]) # if using original datetime col\n",
    "    valid_temporal_ddf = ddf.dropna(subset=required_temporal_cols) # Using derived features\n",
    "    \n",
    "    if len(valid_temporal_ddf) == 0:\n",
    "        logger.warning(\"No valid temporal data after dropping NaNs. Skipping temporal analysis.\")\n",
    "        return {\n",
    "            'hourly': pd.Series(dtype='int64'),\n",
    "            'daily': pd.Series(dtype='int64'),\n",
    "            'monthly': pd.Series(dtype='int64'),\n",
    "            'hour_dow': pd.DataFrame()\n",
    "        }\n",
    "\n",
    "    # Hourly patterns\n",
    "    hourly_trips = valid_temporal_ddf.groupby('pickup_hour').size().compute().sort_index()\n",
    "    \n",
    "    # Daily patterns\n",
    "    daily_trips = valid_temporal_ddf.groupby('pickup_day_of_week').size().compute().sort_index()\n",
    "    \n",
    "    # Monthly patterns\n",
    "    monthly_trips = valid_temporal_ddf.groupby('pickup_month').size().compute().sort_index()\n",
    "    \n",
    "    # Hour vs Day of Week\n",
    "    hour_dow_data = valid_temporal_ddf.groupby(['pickup_hour', 'pickup_day_of_week']).size().compute()\n",
    "    if not hour_dow_data.empty:\n",
    "        hour_dow_data = hour_dow_data.unstack(fill_value=0).sort_index()\n",
    "    else:\n",
    "        hour_dow_data = pd.DataFrame() # Empty dataframe if no data\n",
    "        \n",
    "    return {\n",
    "        'hourly': hourly_trips,\n",
    "        'daily': daily_trips,\n",
    "        'monthly': monthly_trips,\n",
    "        'hour_dow': hour_dow_data\n",
    "    }\n",
    "\n",
    "# Analyze temporal patterns\n",
    "temporal_patterns = analyze_temporal_patterns(processed_ddf)\n",
    "\n",
    "# Create temporal visualizations\n",
    "if not all(val.empty for val in temporal_patterns.values()):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    fig.suptitle('Temporal Patterns in NYC Taxi Trips', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Hourly patterns\n",
    "    if not temporal_patterns['hourly'].empty:\n",
    "        axes[0, 0].bar(temporal_patterns['hourly'].index, temporal_patterns['hourly'].values, \n",
    "                       color='skyblue', alpha=0.8, width=0.8)\n",
    "        axes[0, 0].set_title('Trips by Hour of Day', fontsize=14)\n",
    "        axes[0, 0].set_xlabel('Hour (0-23)', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('Number of Trips', fontsize=12)\n",
    "        axes[0, 0].set_xticks(np.arange(0, 24, 2))\n",
    "        axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    else:\n",
    "        axes[0,0].text(0.5, 0.5, 'No hourly data', horizontalalignment='center', verticalalignment='center', transform=axes[0,0].transAxes)\n",
    "\n",
    "    # Daily patterns\n",
    "    day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    if not temporal_patterns['daily'].empty:\n",
    "        axes[0, 1].bar(temporal_patterns['daily'].index, temporal_patterns['daily'].values, \n",
    "                       color='lightcoral', alpha=0.8, width=0.8)\n",
    "        axes[0, 1].set_title('Trips by Day of Week', fontsize=14)\n",
    "        axes[0, 1].set_xlabel('Day of Week', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('Number of Trips', fontsize=12)\n",
    "        axes[0, 1].set_xticks(range(len(day_names)))\n",
    "        axes[0, 1].set_xticklabels(day_names)\n",
    "        axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    else:\n",
    "        axes[0,1].text(0.5, 0.5, 'No daily data', horizontalalignment='center', verticalalignment='center', transform=axes[0,1].transAxes)\n",
    "\n",
    "    # Monthly patterns\n",
    "    if not temporal_patterns['monthly'].empty:\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        # Ensure index is int for mapping\n",
    "        monthly_idx = temporal_patterns['monthly'].index.astype(int)\n",
    "        axes[1, 0].bar(monthly_idx, temporal_patterns['monthly'].values, \n",
    "                       color='lightgreen', alpha=0.8, width=0.8)\n",
    "        axes[1, 0].set_title('Trips by Month', fontsize=14)\n",
    "        axes[1, 0].set_xlabel('Month', fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Number of Trips', fontsize=12)\n",
    "        axes[1, 0].set_xticks(np.arange(1, 13))\n",
    "        axes[1,0].set_xticklabels([month_names[i-1] for i in np.arange(1,13)], rotation=45)\n",
    "        axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    else:\n",
    "        axes[1,0].text(0.5, 0.5, 'No monthly data', horizontalalignment='center', verticalalignment='center', transform=axes[1,0].transAxes)\n",
    "        \n",
    "    # Heatmap of hour vs day of week\n",
    "    if not temporal_patterns['hour_dow'].empty:\n",
    "        sns.heatmap(temporal_patterns['hour_dow'], ax=axes[1, 1], cmap='YlOrRd', \n",
    "                    xticklabels=day_names, cbar_kws={'label': 'Number of Trips'})\n",
    "        axes[1, 1].set_title('Trips Heatmap: Hour vs Day of Week', fontsize=14)\n",
    "        axes[1, 1].set_xlabel('Day of Week', fontsize=12)\n",
    "        axes[1, 1].set_ylabel('Hour of Day', fontsize=12)\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'No Hour vs DoW data', horizontalalignment='center', verticalalignment='center', transform=axes[1,1].transAxes)\n",
    "        \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\n‚è∞ Temporal Pattern Summary:\")\n",
    "    if not temporal_patterns['hourly'].empty:\n",
    "        print(f\"Peak hour: {temporal_patterns['hourly'].idxmax()}:00 ({temporal_patterns['hourly'].max():,} trips)\")\n",
    "    if not temporal_patterns['daily'].empty:\n",
    "        print(f\"Busiest day: {day_names[temporal_patterns['daily'].idxmax()]} ({temporal_patterns['daily'].max():,} trips)\")\n",
    "    if not temporal_patterns['monthly'].empty:\n",
    "        peak_month_idx = temporal_patterns['monthly'].idxmax()\n",
    "        print(f\"Peak month: {month_names[peak_month_idx-1]} ({temporal_patterns['monthly'].max():,} trips)\")\n",
    "else:\n",
    "    print(\"No temporal data to display or summarize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8120b71",
   "metadata": {},
   "source": [
    "## Geospatial Analysis: H3 Aggregations and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ea5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geospatial Aggregation by H3 Zones\n",
    "def aggregate_by_h3(ddf, h3_resolution):\n",
    "    \"\"\"Aggregate trip counts by H3 zone.\"\"\"\n",
    "    h3_col = f'pickup_h3_r{h3_resolution}'\n",
    "    if ddf is None or h3_col not in ddf.columns:\n",
    "        logger.warning(f\"H3 column {h3_col} not found or DDF is None. Skipping H3 aggregation.\")\n",
    "        return pd.DataFrame(columns=[h3_col, 'trip_count'])\n",
    "        \n",
    "    logger.info(f\"üìä Aggregating trip counts by H3 resolution {h3_resolution}...\")\n",
    "    \n",
    "    # Filter out null H3 values before grouping\n",
    "    valid_h3_ddf = ddf.dropna(subset=[h3_col])\n",
    "    if len(valid_h3_ddf) == 0:\n",
    "        logger.warning(f\"No valid H3 data for resolution {h3_resolution} after dropping NaNs.\")\n",
    "        return pd.DataFrame(columns=[h3_col, 'trip_count'])\n",
    "        \n",
    "    h3_counts = valid_h3_ddf.groupby(h3_col).size().compute()\n",
    "    h3_counts_df = h3_counts.reset_index(name='trip_count').sort_values('trip_count', ascending=False)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Aggregation complete. Found {len(h3_counts_df)} unique H3 zones.\")\n",
    "    return h3_counts_df\n",
    "\n",
    "h3_main_res_col = f'pickup_h3_r{config.H3_MAIN_RESOLUTION}'\n",
    "if processed_ddf is not None and h3_main_res_col in processed_ddf.columns:\n",
    "    h3_aggregated_pickups = aggregate_by_h3(processed_ddf, config.H3_MAIN_RESOLUTION)\n",
    "    print(f\"\\nTop H3 Zones (Resolution {config.H3_MAIN_RESOLUTION}) by Pickup Count:\")\n",
    "    if not h3_aggregated_pickups.empty:\n",
    "        display(h3_aggregated_pickups.head())\n",
    "    else:\n",
    "        print(\"No H3 aggregated data to display.\")\n",
    "else:\n",
    "    print(f\"Skipping H3 aggregation as processed_ddf is None or H3 column '{h3_main_res_col}' is missing.\")\n",
    "    h3_aggregated_pickups = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H3 Hexagon Geometry for Visualization\n",
    "def h3_to_geodataframe(h3_df, h3_col):\n",
    "    \"\"\"Convert a DataFrame with H3 IDs to a GeoDataFrame with hexagon geometries.\"\"\"\n",
    "    if h3_df.empty or h3_col not in h3_df.columns:\n",
    "        logger.warning(\"Input DataFrame for H3 to GeoDataFrame is empty or H3 column missing.\")\n",
    "        return gpd.GeoDataFrame()\n",
    "\n",
    "    def get_h3_boundary(h3_id):\n",
    "        try:\n",
    "            # h3.h3_to_geo_boundary returns list of (lat, lon) tuples\n",
    "            # Polygon expects (lon, lat)\n",
    "            return Polygon([(lon, lat) for lat, lon in h3.h3_to_geo_boundary(h3_id, geo_json=False)])\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error getting boundary for H3 ID {h3_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    h3_df['geometry'] = h3_df[h3_col].apply(get_h3_boundary)\n",
    "    h3_gdf = gpd.GeoDataFrame(h3_df, geometry='geometry', crs='EPSG:4326')\n",
    "    h3_gdf = h3_gdf.dropna(subset=['geometry']) # Remove rows where boundary creation failed\n",
    "    return h3_gdf\n",
    "\n",
    "# Folium Map - H3 Choropleth\n",
    "def plot_h3_choropleth(h3_counts_df, h3_col='pickup_h3_r8', value_col='trip_count', map_title='NYC Taxi Pickups by H3 Zone'):\n",
    "    \"\"\"Create a Folium choropleth map for H3 aggregated data.\"\"\"\n",
    "    if h3_counts_df.empty:\n",
    "        logger.warning(\"H3 counts data is empty. Cannot generate choropleth map.\")\n",
    "        print(\"Cannot generate H3 choropleth: data is empty.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"üó∫Ô∏è Generating H3 Choropleth map for {value_col}...\")\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    h3_gdf = h3_to_geodataframe(h3_counts_df.copy(), h3_col)\n",
    "    if h3_gdf.empty:\n",
    "        logger.warning(\"GeoDataFrame from H3 data is empty. Cannot generate choropleth map.\")\n",
    "        print(\"Cannot generate H3 choropleth: GeoDataFrame is empty after H3 conversion.\")\n",
    "        return None\n",
    "\n",
    "    # Create Folium map centered on NYC\n",
    "    nyc_center = [np.mean([config.NYC_BOUNDS['lat_min'], config.NYC_BOUNDS['lat_max']]), \n",
    "                  np.mean([config.NYC_BOUNDS['lon_min'], config.NYC_BOUNDS['lon_max']])]\n",
    "    m = folium.Map(location=nyc_center, zoom_start=10, tiles='CartoDB positron')\n",
    "\n",
    "    # Add choropleth layer\n",
    "    try:\n",
    "        folium.Choropleth(\n",
    "            geo_data=h3_gdf.to_json(), # Convert GeoDataFrame to GeoJSON string\n",
    "            data=h3_gdf,\n",
    "            columns=[h3_col, value_col],\n",
    "            key_on=f'feature.properties.{h3_col}', # Path to H3 ID in GeoJSON properties\n",
    "            fill_color='YlOrRd',\n",
    "            fill_opacity=0.7,\n",
    "            line_opacity=0.2,\n",
    "            legend_name=f'{value_col.replace(\"_\", \" \").title()}',\n",
    "            name=map_title\n",
    "        ).add_to(m)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating Choropleth layer: {e}\")\n",
    "        print(f\"Error creating Choropleth layer: {e}\")\n",
    "        # Attempt to add individual polygons if Choropleth fails (e.g., due to data scale issues)\n",
    "        # This is a fallback and might be slow for many polygons.\n",
    "        # Normalize data for color mapping\n",
    "        # from matplotlib.colors import Normalize, to_hex\n",
    "        # from matplotlib.cm import YlOrRd\n",
    "        # norm = Normalize(vmin=h3_gdf[value_col].min(), vmax=h3_gdf[value_col].quantile(0.95)) # Cap at 95th percentile for better color scale\n",
    "        # for _, row in h3_gdf.iterrows():\n",
    "        #    folium.GeoJson(\n",
    "        #        row.geometry.__geo_interface__,\n",
    "        #        style_function=lambda x, val=row[value_col]: {\n",
    "        #            'fillColor': to_hex(YlOrRd(norm(val))),\n",
    "        #            'color': 'black',\n",
    "        #            'weight': 0.5,\n",
    "        #            'fillOpacity': 0.7\n",
    "        #        },\n",
    "        #        tooltip=f\"{row[h3_col]}: {row[value_col]}\"\n",
    "        #    ).add_to(m)\n",
    "\n",
    "    folium.LayerControl().add_to(m)\n",
    "    logger.info(\"‚úÖ H3 Choropleth map generated.\")\n",
    "    return m\n",
    "\n",
    "if not h3_aggregated_pickups.empty:\n",
    "    # Take a sample for map visualization if too many H3 cells to prevent browser freezing\n",
    "    sample_h3_map_data = h3_aggregated_pickups.head(2000) # Limit to top N H3s for map\n",
    "    h3_choropleth_map = plot_h3_choropleth(\n",
    "        sample_h3_map_data, \n",
    "        h3_col=f'pickup_h3_r{config.H3_MAIN_RESOLUTION}', \n",
    "        value_col='trip_count',\n",
    "        map_title=f'NYC Taxi Pickups by H3 Zone (Res {config.H3_MAIN_RESOLUTION}) - Top {len(sample_h3_map_data)} zones'\n",
    "    )\n",
    "    if h3_choropleth_map:\n",
    "        display(h3_choropleth_map)\n",
    "else:\n",
    "    print(\"No H3 aggregated data to plot choropleth map.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f779f4",
   "metadata": {},
   "source": [
    "## Geospatial Analysis: Pickup Hotspots and Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1cc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folium Map - Pickup Heatmap and Marker Cluster\n",
    "def plot_pickup_heatmap_markers(ddf, sample_frac_heatmap, sample_frac_markers, lat_col, lon_col):\n",
    "    \"\"\"Create a Folium map with a heatmap and marker clusters for pickup locations.\"\"\"\n",
    "    if ddf is None or len(ddf) == 0:\n",
    "        logger.warning(\"DDF is None or empty. Skipping heatmap/marker plot.\")\n",
    "        print(\"Cannot generate heatmap/markers: data is empty or None.\")\n",
    "        return None\n",
    "    if lat_col not in ddf.columns or lon_col not in ddf.columns:\n",
    "        logger.warning(f\"Lat/Lon columns ('{lat_col}', '{lon_col}') not found. Skipping heatmap/marker plot.\")\n",
    "        print(f\"Cannot generate heatmap/markers: Lat/Lon columns ('{lat_col}', '{lon_col}') not found.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"üó∫Ô∏è Generating Pickup Heatmap and Marker Cluster map...\")\n",
    "    \n",
    "    # Sample data for visualization (Pandas DataFrame)\n",
    "    logger.info(f\"Sampling data for heatmap ({sample_frac_heatmap*100:.2f}%) and markers ({sample_frac_markers*100:.2f}%)...\")\n",
    "    \n",
    "    # Ensure sampling is done on valid coordinates\n",
    "    ddf_coords = ddf[[lat_col, lon_col]].dropna().persist() # Persist small selection\n",
    "    \n",
    "    if len(ddf_coords) == 0:\n",
    "        logger.warning(\"No valid coordinate data after dropping NaNs. Skipping map.\")\n",
    "        print(\"No valid coordinate data for heatmap/markers.\")\n",
    "        return None\n",
    "\n",
    "    sample_heatmap_df = ddf_coords.sample(frac=sample_frac_heatmap, random_state=42).compute()\n",
    "    sample_markers_df = ddf_coords.sample(frac=sample_frac_markers, random_state=123).compute()\n",
    "    \n",
    "    logger.info(f\"Heatmap sample size: {len(sample_heatmap_df)}, Marker sample size: {len(sample_markers_df)}\")\n",
    "\n",
    "    if sample_heatmap_df.empty and sample_markers_df.empty:\n",
    "        logger.warning(\"Sampled data is empty. Cannot generate map.\")\n",
    "        print(\"Sampled data for heatmap/markers is empty.\")\n",
    "        return None\n",
    "\n",
    "    # Create Folium map centered on NYC\n",
    "    nyc_center = [np.mean([config.NYC_BOUNDS['lat_min'], config.NYC_BOUNDS['lat_max']]), \n",
    "                  np.mean([config.NYC_BOUNDS['lon_min'], config.NYC_BOUNDS['lon_max']])]\n",
    "    m = folium.Map(location=nyc_center, zoom_start=11, tiles='CartoDB positron')\n",
    "\n",
    "    # Add Heatmap layer\n",
    "    if not sample_heatmap_df.empty:\n",
    "        heat_data = [[row[lat_col], row[lon_col]] for index, row in sample_heatmap_df.iterrows()]\n",
    "        HeatMap(heat_data, radius=10, blur=15, name=\"Pickup Heatmap\").add_to(m)\n",
    "    \n",
    "    # Add Marker Cluster layer (FastMarkerCluster for better performance)\n",
    "    if not sample_markers_df.empty:\n",
    "        marker_locations = [[row[lat_col], row[lon_col]] for index, row in sample_markers_df.iterrows()]\n",
    "        # Using FastMarkerCluster for potentially large number of points\n",
    "        FastMarkerCluster(data=marker_locations, name=\"Pickup Clusters (Sampled)\").add_to(m)\n",
    "        # Alternative: standard MarkerCluster\n",
    "        # marker_cluster = MarkerCluster(name=\"Pickup Clusters (Sampled)\").add_to(m)\n",
    "        # for lat, lon in marker_locations:\n",
    "        #     folium.Marker([lat, lon]).add_to(marker_cluster)\n",
    "\n",
    "    # Add Borough boundaries if available\n",
    "    if 'boroughs' in geo_data and not geo_data['boroughs'].empty:\n",
    "        folium.GeoJson(\n",
    "            geo_data['boroughs'],\n",
    "            name='NYC Boroughs',\n",
    "            style_function=lambda x: {'fillColor': 'gray', 'color': 'black', 'weight': 1, 'fillOpacity': 0.1}\n",
    "        ).add_to(m)\n",
    "        \n",
    "    folium.LayerControl().add_to(m)\n",
    "    logger.info(\"‚úÖ Pickup Heatmap and Marker Cluster map generated.\")\n",
    "    return m\n",
    "\n",
    "if processed_ddf is not None:\n",
    "    pickup_heatmap_map = plot_pickup_heatmap_markers(\n",
    "        processed_ddf, \n",
    "        sample_frac_heatmap=config.SAMPLE_RATES.get('heatmap', 0.01), \n",
    "        sample_frac_markers=config.SAMPLE_RATES.get('visualization', 0.001), # Smaller sample for markers\n",
    "        lat_col=config.PICKUP_LAT_COL, \n",
    "        lon_col=config.PICKUP_LON_COL\n",
    "    )\n",
    "    if pickup_heatmap_map:\n",
    "        display(pickup_heatmap_map)\n",
    "else:\n",
    "    print(\"Skipping pickup heatmap/marker map as processed_ddf is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Geospatial Analysis - DBSCAN Clustering for Hotspots\n",
    "def find_hotspots_dbscan(ddf, sample_frac, lat_col, lon_col, eps=0.001, min_samples=50):\n",
    "    \"\"\"Find hotspots using DBSCAN clustering on a sample of pickup locations.\"\"\"\n",
    "    if ddf is None or len(ddf) == 0:\n",
    "        logger.warning(\"DDF is None or empty. Skipping DBSCAN clustering.\")\n",
    "        return None, pd.DataFrame()\n",
    "    if lat_col not in ddf.columns or lon_col not in ddf.columns:\n",
    "        logger.warning(f\"Lat/Lon columns ('{lat_col}', '{lon_col}') not found. Skipping DBSCAN clustering.\")\n",
    "        return None, pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"üî• Finding hotspots using DBSCAN (eps={eps}, min_samples={min_samples})...\")\n",
    "    logger.info(f\"Sampling {sample_frac*100:.2f}% of data for DBSCAN...\")\n",
    "\n",
    "    # Sample data and select coordinates, drop NaNs\n",
    "    coords_ddf = ddf[[lat_col, lon_col]].dropna().persist()\n",
    "    if len(coords_ddf) == 0:\n",
    "        logger.warning(\"No valid coordinate data after dropping NaNs for DBSCAN.\")\n",
    "        return None, pd.DataFrame()\n",
    "        \n",
    "    sample_df = coords_ddf.sample(frac=sample_frac, random_state=42).compute()\n",
    "    if sample_df.empty:\n",
    "        logger.warning(\"Sampled data for DBSCAN is empty.\")\n",
    "        return None, pd.DataFrame()\n",
    "    \n",
    "    logger.info(f\"DBSCAN on {len(sample_df)} points.\")\n",
    "    coords = sample_df[[lat_col, lon_col]].values\n",
    "\n",
    "    # Scale data (important for distance-based algorithms like DBSCAN)\n",
    "    # Note: Scaling lat/lon might distort geographic distances if not handled carefully.\n",
    "    # For DBSCAN, 'eps' is in the units of the input data. If using raw lat/lon,\n",
    "    # eps=0.001 is approx 111 meters in latitude, variable in longitude.\n",
    "    # scaler = StandardScaler()\n",
    "    # coords_scaled = scaler.fit_transform(coords)\n",
    "    # db = DBSCAN(eps=eps_scaled, min_samples=min_samples).fit(coords_scaled) # eps needs adjustment if scaled\n",
    "\n",
    "    # Using Haversine distance for DBSCAN for geographic data\n",
    "    # Convert degrees to radians for haversine metric\n",
    "    kms_per_radian = 6371.0088 \n",
    "    # eps in kilometers, convert to radians for haversine\n",
    "    # Example: if eps is 0.1 km (100 meters)\n",
    "    epsilon_geo = (eps / kms_per_radian) \n",
    "\n",
    "    db = DBSCAN(eps=epsilon_geo, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "    \n",
    "    sample_df['cluster'] = db.labels_\n",
    "    num_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)\n",
    "    logger.info(f\"‚úÖ DBSCAN complete. Found {num_clusters} clusters and { (db.labels_ == -1).sum()} noise points.\")\n",
    "\n",
    "    # Plot clusters on a map\n",
    "    if num_clusters > 0:\n",
    "        nyc_center = [np.mean([config.NYC_BOUNDS['lat_min'], config.NYC_BOUNDS['lat_max']]), \n",
    "                      np.mean([config.NYC_BOUNDS['lon_min'], config.NYC_BOUNDS['lon_max']])]\n",
    "        map_clusters = folium.Map(location=nyc_center, zoom_start=11, tiles='CartoDB positron')\n",
    "        \n",
    "        # Create a color palette for clusters\n",
    "        # Exclude noise points (cluster == -1)\n",
    "        unique_labels = sorted(sample_df['cluster'].unique())\n",
    "        unique_labels = [l for l in unique_labels if l != -1]\n",
    "        colors = sns.color_palette(\"husl\", len(unique_labels)).as_hex()\n",
    "        cluster_colors = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "\n",
    "        for index, row in sample_df.iterrows():\n",
    "            if row['cluster'] != -1: # Only plot actual clusters, not noise\n",
    "                folium.CircleMarker(\n",
    "                    location=[row[lat_col], row[lon_col]],\n",
    "                    radius=3,\n",
    "                    color=cluster_colors[row['cluster']],\n",
    "                    fill=True,\n",
    "                    fill_color=cluster_colors[row['cluster']],\n",
    "                    fill_opacity=0.7,\n",
    "                    tooltip=f\"Cluster {row['cluster']}\"\n",
    "                ).add_to(map_clusters)\n",
    "        return map_clusters, sample_df[sample_df['cluster'] != -1]\n",
    "    else:\n",
    "        logger.info(\"No clusters found by DBSCAN.\")\n",
    "        return None, sample_df\n",
    "\n",
    "if processed_ddf is not None:\n",
    "    # Adjust eps (in km) and min_samples based on data density and desired granularity\n",
    "    # eps=0.1 means points within 100m (approx) can belong to the same cluster.\n",
    "    dbscan_map, clustered_data = find_hotspots_dbscan(\n",
    "        processed_ddf, \n",
    "        sample_frac=config.SAMPLE_RATES.get('clustering', 0.005), \n",
    "        lat_col=config.PICKUP_LAT_COL, \n",
    "        lon_col=config.PICKUP_LON_COL,\n",
    "        eps=0.1, # Epsilon in kilometers (e.g., 0.1 km = 100 meters)\n",
    "        min_samples=20 \n",
    "    )\n",
    "    if dbscan_map:\n",
    "        print(\"üó∫Ô∏è DBSCAN Hotspot Clusters Map:\")\n",
    "        display(dbscan_map)\n",
    "        print(\"\\nSample of Clustered Data (excluding noise):\")\n",
    "        display(clustered_data.head())\n",
    "        print(f\"\\nCluster sizes:\\n{clustered_data['cluster'].value_counts().sort_index()}\")\n",
    "    else:\n",
    "        print(\"DBSCAN did not produce a map (e.g., no clusters found or data was insufficient).\")\n",
    "else:\n",
    "    print(\"Skipping DBSCAN clustering as processed_ddf is None.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48fe4b8",
   "metadata": {},
   "source": [
    "## Trip Characteristics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip Characteristics Analysis (e.g., distance, fare, passenger count)\n",
    "def analyze_trip_characteristics(ddf):\n",
    "    \"\"\"Analyze distributions of key trip characteristics.\"\"\"\n",
    "    if ddf is None or len(ddf) == 0:\n",
    "        logger.warning(\"DDF is None or empty. Skipping trip characteristics analysis.\")\n",
    "        return {}\n",
    "\n",
    "    logger.info(\"üìä Analyzing trip characteristics...\")\n",
    "    \n",
    "    characteristics = {}\n",
    "    cols_to_analyze = {\n",
    "        'trip_distance': {'range': (0, 50)}, # miles, filter outliers for viz\n",
    "        'fare_amount': {'range': (0, 100)}, # USD, filter outliers for viz\n",
    "        'total_amount': {'range': (0, 150)}, # USD, filter outliers for viz\n",
    "        'passenger_count': {'range': (0, 8)} # filter outliers for viz\n",
    "    }\n",
    "    \n",
    "    # Compute basic stats for relevant columns\n",
    "    desc_cols = [col for col in cols_to_analyze if col in ddf.columns]\n",
    "    if not desc_cols:\n",
    "        logger.warning(\"No characteristic columns found for analysis.\")\n",
    "        return {}\n",
    "        \n",
    "    # Persist relevant subset for multiple computations\n",
    "    ddf_subset = ddf[desc_cols].dropna().persist()\n",
    "    if len(ddf_subset) == 0:\n",
    "        logger.warning(\"No data available for trip characteristics after dropping NaNs.\")\n",
    "        return {}\n",
    "        \n",
    "    characteristics['summary_stats'] = ddf_subset.describe().compute()\n",
    "\n",
    "    # For histograms, sample data to avoid OOM with large datasets if not using Dask-Plotly\n",
    "    # Or compute histograms with Dask then plot with Matplotlib/Seaborn\n",
    "    sample_for_hist_df = ddf_subset.sample(frac=0.1, random_state=1).compute() # 10% sample\n",
    "    if sample_for_hist_df.empty:\n",
    "        logger.warning(\"Sampled data for histograms is empty.\")\n",
    "        return characteristics # Return summary stats at least\n",
    "        \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle('Distribution of Trip Characteristics (Sampled Data)', fontsize=18, fontweight='bold')\n",
    "\n",
    "    for i, (col, props) in enumerate(cols_to_analyze.items()):\n",
    "        if col in sample_for_hist_df.columns:\n",
    "            data_to_plot = sample_for_hist_df[col]\n",
    "            # Apply range filtering for visualization clarity\n",
    "            if 'range' in props:\n",
    "                data_to_plot = data_to_plot[(data_to_plot >= props['range'][0]) & (data_to_plot <= props['range'][1])]\n",
    "            \n",
    "            if not data_to_plot.empty:\n",
    "                sns.histplot(data_to_plot, kde=False, ax=axes[i], bins=50, color=sns.color_palette(\"viridis\", 4)[i])\n",
    "                axes[i].set_title(f'Distribution of {col.replace(\"_\", \" \").title()}', fontsize=14)\n",
    "                axes[i].set_xlabel(col.replace(\"_\", \" \").title(), fontsize=12)\n",
    "                axes[i].set_ylabel('Frequency', fontsize=12)\n",
    "                axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, f'No data for {col}', horizontalalignment='center', verticalalignment='center', transform=axes[i].transAxes)\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, f'{col} not found', horizontalalignment='center', verticalalignment='center', transform=axes[i].transAxes)\n",
    "            \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(\"‚úÖ Trip characteristics analysis complete.\")\n",
    "    return characteristics\n",
    "\n",
    "if processed_ddf is not None:\n",
    "    trip_char_analysis = analyze_trip_characteristics(processed_ddf)\n",
    "    if 'summary_stats' in trip_char_analysis and not trip_char_analysis['summary_stats'].empty:\n",
    "        print(\"\\nüìä Summary Statistics of Trip Characteristics:\")\n",
    "        display(trip_char_analysis['summary_stats'])\n",
    "    else:\n",
    "        print(\"No summary statistics for trip characteristics to display.\")\n",
    "else:\n",
    "    print(\"Skipping trip characteristics analysis as processed_ddf is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "def analyze_correlations(ddf):\n",
    "    \"\"\"Compute and visualize correlation matrix for numerical features.\"\"\"\n",
    "    if ddf is None or len(ddf) == 0:\n",
    "        logger.warning(\"DDF is None or empty. Skipping correlation analysis.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(\"üîó Analyzing correlations...\")\n",
    "    \n",
    "    # Select numerical columns for correlation\n",
    "    numerical_cols = ddf.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Remove ID columns or others not suitable for direct correlation conceptually\n",
    "    cols_to_exclude = ['VendorID', 'RatecodeID', 'payment_type', \n",
    "                       'pickup_hour', 'pickup_day_of_week', 'pickup_month'] # Temporal features are categorical-like\n",
    "    cols_for_corr = [col for col in numerical_cols if col not in cols_to_exclude and \n",
    "                     config.PICKUP_LAT_COL not in col and config.PICKUP_LON_COL not in col and \n",
    "                     config.DROPOFF_LAT_COL not in col and config.DROPOFF_LON_COL not in col]\n",
    "\n",
    "    if len(cols_for_corr) < 2:\n",
    "        logger.warning(f\"Not enough numerical columns ({cols_for_corr}) for correlation analysis.\")\n",
    "        return None\n",
    "        \n",
    "    # Compute correlation matrix (on a sample or full data if small enough)\n",
    "    # Dask's .corr() can be memory intensive. For large data, sampling is advised.\n",
    "    # Here, we try on full data, assuming 'processed_ddf' might be persisted or manageable.\n",
    "    # If it's too large, this will be slow or fail. Consider sampling:\n",
    "    # sample_corr_df = ddf[cols_for_corr].sample(frac=0.1).compute()\n",
    "    # corr_matrix = sample_corr_df.corr()\n",
    "    try:\n",
    "        logger.info(f\"Computing correlation matrix for columns: {cols_for_corr}\")\n",
    "        corr_matrix = ddf[cols_for_corr].corr().compute()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing correlation matrix: {e}. Trying with a sample.\")\n",
    "        try:\n",
    "            sample_corr_df = ddf[cols_for_corr].dropna().sample(frac=0.01, random_state=1).compute()\n",
    "            if len(sample_corr_df) < 2:\n",
    "                 logger.error(\"Sample for correlation is too small.\")\n",
    "                 return None\n",
    "            corr_matrix = sample_corr_df.corr()\n",
    "        except Exception as e_sample:\n",
    "            logger.error(f\"Error computing correlation matrix on sample: {e_sample}\")\n",
    "            return None\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "    plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(\"‚úÖ Correlation analysis complete.\")\n",
    "    return corr_matrix\n",
    "\n",
    "if processed_ddf is not None:\n",
    "    correlation_matrix = analyze_correlations(processed_ddf)\n",
    "    if correlation_matrix is not None:\n",
    "        print(\"\\nüîó Correlation Matrix:\")\n",
    "        display(correlation_matrix)\n",
    "    else:\n",
    "        print(\"Correlation analysis failed or no suitable data.\")\n",
    "else:\n",
    "    print(\"Skipping correlation analysis as processed_ddf is None.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211c576",
   "metadata": {},
   "source": [
    "## Interactive Visualizations with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Plotly Visualizations\n",
    "def plotly_trip_analysis(ddf, sample_frac=0.01):\n",
    "    \"\"\"Create interactive Plotly visualizations for trip analysis.\"\"\"\n",
    "    if ddf is None or len(ddf) == 0:\n",
    "        logger.warning(\"DDF is None or empty. Skipping Plotly visualizations.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"üé® Generating interactive Plotly visualizations (sample_frac={sample_frac})...\")\n",
    "    \n",
    "    # Columns needed for plots\n",
    "    plot_cols = ['trip_distance', 'fare_amount', 'passenger_count', 'pickup_hour']\n",
    "    # Check if essential columns exist\n",
    "    if not all(col in ddf.columns for col in ['trip_distance', 'fare_amount']):\n",
    "        logger.warning(\"Essential columns for Plotly viz (trip_distance, fare_amount) are missing.\")\n",
    "        return\n",
    "        \n",
    "    # Sample data for Plotly (Pandas DataFrame for easier use with Plotly Express)\n",
    "    # Persist subset before sampling\n",
    "    ddf_subset = ddf[[col for col in plot_cols if col in ddf.columns]].dropna().persist()\n",
    "    if len(ddf_subset) == 0:\n",
    "        logger.warning(\"No data for Plotly viz after dropping NaNs.\")\n",
    "        return\n",
    "        \n",
    "    sample_df = ddf_subset.sample(frac=sample_frac, random_state=42).compute()\n",
    "    if sample_df.empty:\n",
    "        logger.warning(\"Sampled data for Plotly is empty.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Plotting with {len(sample_df)} sampled rows.\")\n",
    "\n",
    "    # 1. Scatter plot: Trip Distance vs. Fare Amount\n",
    "    if 'trip_distance' in sample_df.columns and 'fare_amount' in sample_df.columns:\n",
    "        fig1_title = 'Trip Distance vs. Fare Amount'\n",
    "        color_col = 'passenger_count' if 'passenger_count' in sample_df.columns else None\n",
    "        \n",
    "        # Filter for reasonable values to make plot readable\n",
    "        plot_df_fig1 = sample_df[\n",
    "            (sample_df['trip_distance'] > 0) & (sample_df['trip_distance'] < 50) &\n",
    "            (sample_df['fare_amount'] > 0) & (sample_df['fare_amount'] < 200)\n",
    "        ]\n",
    "        if not plot_df_fig1.empty:\n",
    "            fig1 = px.scatter(\n",
    "                plot_df_fig1, \n",
    "                x='trip_distance', \n",
    "                y='fare_amount', \n",
    "                color=color_col,\n",
    "                title=fig1_title,\n",
    "                labels={'trip_distance': 'Trip Distance (miles)', 'fare_amount': 'Fare Amount (USD)'},\n",
    "                opacity=0.5,\n",
    "                hover_data=plot_df_fig1.columns\n",
    "            )\n",
    "            fig1.show()\n",
    "        else:\n",
    "            logger.warning(f\"No data for '{fig1_title}' after filtering.\")\n",
    "\n",
    "    # 2. Box plot: Fare Amount by Hour of Day\n",
    "    if 'fare_amount' in sample_df.columns and 'pickup_hour' in sample_df.columns:\n",
    "        fig2_title = 'Fare Amount Distribution by Pickup Hour'\n",
    "        plot_df_fig2 = sample_df[\n",
    "            (sample_df['fare_amount'] > 0) & (sample_df['fare_amount'] < 100)\n",
    "        ]\n",
    "        if not plot_df_fig2.empty:\n",
    "            fig2 = px.box(\n",
    "                plot_df_fig2, \n",
    "                x='pickup_hour', \n",
    "                y='fare_amount', \n",
    "                title=fig2_title,\n",
    "                labels={'pickup_hour': 'Hour of Day', 'fare_amount': 'Fare Amount (USD)'},\n",
    "                color='pickup_hour'\n",
    "            )\n",
    "            fig2.update_xaxes(type='category') # Treat hour as categorical\n",
    "            fig2.show()\n",
    "        else:\n",
    "            logger.warning(f\"No data for '{fig2_title}' after filtering.\")\n",
    "            \n",
    "    # 3. 3D Scatter plot of pickup locations (if lat/lon and a value like fare_amount exist)\n",
    "    # This requires more data and specific columns like pickup_latitude, pickup_longitude\n",
    "    # For this example, we'll skip it to keep it general, but here's a template:\n",
    "    # if all(c in sample_df.columns for c in [config.PICKUP_LAT_COL, config.PICKUP_LON_COL, 'fare_amount']):\n",
    "    #    fig3 = px.scatter_3d(sample_df, x=config.PICKUP_LON_COL, y=config.PICKUP_LAT_COL, z='fare_amount',\n",
    "    #                         color='fare_amount', title='3D Pickup Locations and Fare Amount')\n",
    "    #    fig3.show()\n",
    "    \n",
    "    logger.info(\"‚úÖ Plotly visualizations generated.\")\n",
    "\n",
    "if processed_ddf is not None:\n",
    "    plotly_trip_analysis(processed_ddf, sample_frac=config.SAMPLE_RATES.get('visualization', 0.01))\n",
    "else:\n",
    "    print(\"Skipping Plotly visualizations as processed_ddf is None.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d652f3",
   "metadata": {},
   "source": [
    "## Conclusion and Key Findings\n",
    "\n",
    "This notebook performed an enhanced exploratory data analysis of the NYC Taxi dataset, leveraging Dask for scalable computation, GeoPandas and H3 for geospatial analysis, and various libraries for advanced visualization.\n",
    "\n",
    "**Key steps and observations include:**\n",
    "\n",
    "1.  **Data Loading and Preprocessing:** Large datasets were efficiently handled using Dask. Data types were enforced, and datetime conversions were performed. H3 hexagonal indexing was added at multiple resolutions to facilitate geospatial aggregation, alongside temporal feature engineering (hour, day of week, month).\n",
    "\n",
    "2.  **Data Quality Assessment:** Missing values and coordinate validity were assessed. This step is crucial for understanding data limitations and ensuring robustness of subsequent analyses. (Actual percentages depend on the specific dataset used).\n",
    "\n",
    "3.  **Temporal Analysis:**\n",
    "    *   Pickup patterns by hour, day of week, and month were analyzed, revealing peak and off-peak times. \n",
    "    *   A heatmap of trips by hour and day of the week provided a detailed view of demand fluctuations.\n",
    "    *   *Specific findings (e.g., peak hours typically in the late afternoon/evening, higher demand on weekdays) would be listed here based on the actual data.*\n",
    "\n",
    "4.  **Geospatial Analysis:**\n",
    "    *   **H3 Aggregations:** Trip counts were aggregated by H3 zones, identifying high-traffic hexagonal areas. These were visualized on a choropleth map, offering a granular view of pickup density across NYC.\n",
    "    *   **Pickup Hotspots:** Folium heatmaps and marker clusters visualized general pickup concentrations. \n",
    "    *   **DBSCAN Clustering:** A more advanced technique, DBSCAN, was applied to a sample of pickup locations to identify statistically significant clusters (hotspots), which were then plotted on an interactive map. This helps in pinpointing specific areas of high taxi activity.\n",
    "\n",
    "5.  **Trip Characteristics Analysis:**\n",
    "    *   Distributions of `trip_distance`, `fare_amount`, `total_amount`, and `passenger_count` were examined. \n",
    "    *   *Observations might include typical trip distances being short, common fare ranges, and most trips having 1-2 passengers.*\n",
    "    *   A correlation matrix highlighted relationships between numerical features (e.g., strong correlation between `trip_distance` and `fare_amount`).\n",
    "\n",
    "6.  **Interactive Visualizations:** Plotly Express was used to create interactive charts, such as scatter plots of trip distance vs. fare, and box plots of fare by hour, allowing for dynamic exploration of the data.\n",
    "\n",
    "**Overall Insights:**\n",
    "*The combination of temporal and geospatial analysis provides a rich understanding of taxi usage patterns. Identifying hotspots and peak times can be valuable for resource allocation, urban planning, and business strategy for taxi services.*\n",
    "\n",
    "**Future Work:**\n",
    "*   Origin-Destination (OD) analysis between H3 zones.\n",
    "*   Analysis of dropoff patterns.\n",
    "*   Integration with external datasets (e.g., weather, public events) to explore influencing factors.\n",
    "*   Predictive modeling for demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Dask client and cluster\n",
    "def cleanup_dask():\n",
    "    \"\"\"Close Dask client and cluster if they exist.\"\"\"\n",
    "    logger.info(\"üßπ Cleaning up Dask resources...\")\n",
    "    global client, cluster\n",
    "    try:\n",
    "        if 'client' in globals() and client:\n",
    "            client.close()\n",
    "            logger.info(\"Dask client closed.\")\n",
    "        if 'cluster' in globals() and cluster:\n",
    "            cluster.close()\n",
    "            logger.info(\"Dask cluster closed.\")\n",
    "    except NameError:\n",
    "        logger.info(\"No Dask client or cluster found to close.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during Dask cleanup: {e}\")\n",
    "\n",
    "cleanup_dask()\n",
    "print(\"‚úÖ Notebook execution complete and Dask resources cleaned up (if initialized).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
